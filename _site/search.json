[
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Carl Goodwin",
    "section": "",
    "text": "I live in London and post-graduated with an MBA from Hult International Business School as valedictorian in 2001.\nI served for 36 years at Big Blue, performing roles combining business (strategy and growth) leadership with data science and machine learning.\nMy passion for data science and machine learning extends beyond work. The tidyverse and tidymodels make this a real joy to do.\nAdobe Fresco does the same for my little digital artworks  to bring it all to life."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Night Mode\n\n\n2 min\n\n\nMaking full use of Quarto and sprucing up an oft-visited 404 page\n\n\n\nNov 22, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPainting Tails\n\n\n2 min\n\n\nIf you’re a cat, go find the nearest open pot of paint. But if you’re a data scientist, what to do?\n\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Fresh Lick of Paint\n\n\n5 min\n\n\nStaying in Blogdown and renovating with the Hugo Apéro theme\n\n\n\nApr 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMoving House\n\n\n3 min\n\n\nLeaving Wordpress for a quieter life in Blogdown with the Hugo Academic theme\n\n\n\nJul 26, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/tail/index.html",
    "href": "blog/tail/index.html",
    "title": "Painting Tails",
    "section": "",
    "text": "There are techniques for painting a region under a curve. But the experimental ggfx package offers an interesting alternative solution based on the blending modes familiar to users of Photoshop.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(ggfx)\nlibrary(patchwork)\nlibrary(wesanderson)\nlibrary(clock)\nlibrary(tidyquant)\n\n\ntheme_set(theme_bw())\n\n(cols <- wes_palette(\"Royal1\"))\n\n\n\n\nThe advantage here is that the tail-painting aesthetic needs no information about the shape of the curve; only the limits on the x-axis.\nThe left plot shows the raw components without blending. The right plot is only retaining the red where there is a layer below.\n\np0 <- tibble(outcome = rnorm(10000, 20, 2)) |>\n  ggplot(aes(outcome)) +\n  scale_y_continuous(labels = label_percent())\n\np1 <- p0 +\n  geom_density(adjust = 2, fill = cols[3]) +\n  annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ) + \n  labs(title = \"Without Blending\", y = \"Density\")\n\np2 <- p0 +\n  as_reference(geom_density(adjust = 2, fill = cols[3]), id = \"density\") +\n  with_blend(annotate(\"rect\",\n    xmin = 15, xmax = 18, ymin = -Inf, ymax = Inf,\n    fill = cols[2]\n  ), bg_layer = \"density\", blend_type = \"atop\") + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2\n\n\n\n\nOf course the red box could also be layered behind a density curve with alpha applied so it shows through. But if the preference is tail-only colouring, it’s a neat solution.\nBlending is actually a handy solution for any awkward shape. The same technique is used here with a time series ribbon summarising the median, lower and upper quartiles of a set of closing stock prices.\n\n\n\n\n\n\nNote\n\n\n\nTry this patch if having problems with tq_get\nThis chunk is using the development version of dplyr which introduces temporary grouping with .by.\n\n\n\ntickrs <- c(\"AAPL\", \"NFLX\", \"TSLA\", \"ADBE\", \"META\", \"GOOG\", \"MSFT\")\n\np0 <- tq_get(tickrs, get = \"stock.prices\", from = \"2022-01-01\") |>\n  filter(!is.na(close)) |> \n  reframe(\n    close = quantile(close, c(0.25, 0.5, 0.75)),\n    quantile = c(\"lower\", \"median\", \"upper\") |> factor(),\n    .by = date\n  ) |>\n  pivot_wider(names_from = quantile, values_from = close) |>\n  ggplot(aes(date, median)) +\n  annotate(\"text\",\n    x = as.Date(\"2022-03-16\"), y = 100,\n    label = \"Helpful\\nAnnotation\", colour = \"black\"\n  ) +\n  scale_y_continuous(limits = c(0, NA)) +\n  labs(x = NULL)\n\np1 <- p0 +\n  geom_ribbon(aes(ymin = lower, ymax = upper), fill = cols[1]) +\n  geom_line(colour = cols[3]) +\n  annotate(\"rect\",\n    xmin = as.Date(\"2022-03-01\"), xmax = as.Date(\"2022-03-31\"),\n    ymin = -Inf, ymax = Inf, fill = cols[4], colour = \"black\", linetype = \"dashed\"\n  ) + \n  labs(title = \"Without Blending\", y = \"Closing Price\")\n\np2 <- p0 +\n  as_reference(geom_ribbon(aes(ymin = lower, ymax = upper), fill = cols[1]), id = \"ribbon\") +\n  with_blend(\n    annotate(\n      \"rect\",\n      xmin = as.Date(\"2022-03-01\"), xmax = as.Date(\"2022-03-31\"),\n      ymin = -Inf, ymax = Inf, fill = cols[4], colour = \"black\", linetype = \"dashed\"\n      ),\n    bg_layer = \"ribbon\", blend_type = \"atop\"\n    ) +\n  geom_line(colour = cols[3]) + \n  labs(title = \"With Blending\", y = NULL)\n\np1 + p2 +\n  plot_annotation(title = \"Median Price Bounded by Upper & Lower Quartiles\")"
  },
  {
    "objectID": "blog/dark/index.html",
    "href": "blog/dark/index.html",
    "title": "Night Mode",
    "section": "",
    "text": "It was only last April that I renovated my blog as described in A Fresh Lick of Paint. Following the launch of a new open-source scientific and technical publishing system, it’s time to get the paint brush out again.\nI initially started to convert some of my projects and posts from Rmarkdown to qmd using format: hugo-md. The thought was to keep my existing site aesthetic, with its beautiful Hugo Apéro theme, unchanged. I found though that it wasn’t possible to make full use of the myriad features offered by Quarto. For example code-link: true and date-modified: last-modified. This is because the document metadata is preserved as-is for formats like Hugo.\nSo, I decided to start building a “full-on Quarto” version, i.e. format: html, with the intent of switching the website over only if I preferred the new versus the old when running the two side-by-side.\nI’ve switched over.\nI’ve given the site a unique look-and-feel by customising the flatly and darkly themes with a number of SASS variables in two theme.scss files, for example, to adopt my own reversible colour scheme.\nThe landing page switches image based on the dark-mode setting by making the class of the first image .dark-mode and adding these two lines in the dark theme’s custom scss file:\n.dark-mode { display: block; }\n.light-mode { display: none; }\nThen the second image has the class .light-mode and uses the mirror css code in the light theme’s custom scss file:\n.light-mode { display: block; }\n.dark-mode { display: none; }\nFor the navbar logo, my initial idea was to have a mid-grey logo which darkens and lightens by adjusting the brightness based on the mode:\n.navbar-logo {\n    filter: brightness(2);\n    max-height: 30px;\n}\n\n.navbar-logo {\n    filter: brightness(20%);\n    max-height: 30px;\n}\nThis worked nicely for some browsers, e.g. firefox, but not others, e.g. safari; perhaps because the cache is cleared for some but not all. I’ve instead implemented a dark-mode logo using a background image in dark.scss1.\nAs for other features, not only does the site now have the code-link, date-modified, and dark-mode, but it also restores the grid-based listing pages given up when moving from Hugo Academic. Many other Quarto features such as call-outs, citations, footnotes and freeze are also now deployed.\nLua Filters are a powerful tool. The _quarto.yml file includes one to check my current installed version of Quarto and then insert that into the website’s page-footer2.\nWhilst refreshing the site, I took the opportunity to make my 404 page a little more welcoming as it seemed to be one of my more popular pages. Feel free to try it by visiting an imaginatively made-up page of your choice.\nThe updated repo is public on github.\n\n\n\n\nFootnotes\n\n\nAs suggested in Quarto Discussions↩︎\nSuggested solution in Stack Overflow↩︎"
  },
  {
    "objectID": "blog/renovate/index.html#motivation",
    "href": "blog/renovate/index.html#motivation",
    "title": "A Fresh Lick of Paint",
    "section": "Motivation",
    "text": "Motivation\nA couple of years ago I moved house from Wordpress to Blogdown. It’s proved to be a much less stressful life and I plan to stay. Hugo Academic served me well, but sometimes you just need a fresh coat of paint. I liked the look of Hugo Apéro.\nApéro feels simpler and has an elegant design with well-chosen themes and fonts.\nI like to add my own digital art to both the site and Rmarkdown projects, and Apéro gives me more flexibility here. GIF animations, for example, on my home page and in my project and blog lists just work.\nThe dark mode I had with Academic would be a nice-to-have, but not essential."
  },
  {
    "objectID": "blog/renovate/index.html#plan-of-attack",
    "href": "blog/renovate/index.html#plan-of-attack",
    "title": "A Fresh Lick of Paint",
    "section": "Plan of Attack",
    "text": "Plan of Attack\nThe upgrade approach I took was to create a brand new blogdown project in RStudio with the Apéro theme and then copy over and re-knit my projects one by one. This worked well because every project needed at least one change as a direct consequence of the move and re-opening each project also prompted other beneficial updates.\nI focused first on manual deployment, i.e. dragging the Public folder to Netlify, rather than going straight to continuous deployment via Github. Doing it this way would narrow the potential cause of any problems when doing the latter. I also initially deployed to one of Netlify’s auto-generated site names, so my live manually-deployed Academic blog remained unaffected whilst preparing the new site."
  },
  {
    "objectID": "blog/renovate/index.html#set-up",
    "href": "blog/renovate/index.html#set-up",
    "title": "A Fresh Lick of Paint",
    "section": "Set-up",
    "text": "Set-up\nThere’s a very helpful get started authored by the theme owner Alison Hill, so I’ll comment here only on the personal touches I wanted to add.\nIn Hugo Academic, each project’s (or post’s) feature image rendered automatically in both the project list page and in the individual project. In Apéro, I needed to add ![](pathname) to the Rmarkdown file to render the image in an individual project or post. I actually prefer this approach because it means the image then also appears when re-publishing to a blog aggregator which frustratingly was not the case with Academic.\nGiven the taxonomy differences, I created a static/_redirects file so that bookmarks for, say, category/r or tag/statistical-inference (under Academic) would go to categories/r or tags/statistical-inference.\n\n\n/index.xml                          /project/index-R.xml\n/categories/r/index.xml             /project/index-R.xml\n\n\nI had customised my Academic site to show the updated, as well as posted, date for each project and post. So to get the same in Apéro, I copied the themes > hugo-apero > layouts > partials > shared > post-details.html file to layouts > partials > shared > post-details.html, duplicated lines 2-5 below and changed .PublishDate to .Lastmod. As my YAML header for all projects and posts already included lastmod:, the details twistie at the foot of each project (and post) now shows both dates.\n<details {{ .Scratch.Get \"details\" }} class=\"f6 fw7 input-reset\">\n  <dl class=\"f6 lh-copy\">\n    <dt class=\"fw7\">Posted:</dt>\n    <dd class=\"fw5 ml0\">{{ .PublishDate.Format \"January 2, 2006\" }}</dd>\n  </dl>\n  <dl class=\"f6 lh-copy\">\n    <dt class=\"fw7\">Updated:</dt>\n    <dd class=\"fw5 ml0\">{{ .Lastmod.Format \"January 2, 2006\" }}</dd>\n  </dl>\nI used a tag cloud in Academic and wanted to replicate this too. To do so, I also copied the themes > hugo-apero > layouts > partials > shared > summary-li.html file to layouts > partials > shared and changed the last section to refer to tags rather than categories. I removed most of the other code to simplify the About page, so my customised summary-li.html contained only the code below. This change also required a tweak to the content > about > main > index.md to replace number_categories: with a number_tags: parameter.\n<section class=\"featured-content\">\n{{ $page := . }} <!--save current page-->\n\n{{ $number_tags := $page.Params.number_tags | default 0 }}\n{{ if ge $number_tags 1 }}\n  <article{{ if .Params.show_outro }} class=\"bb pb5\"{{ end }}>\n  <h5 class=\"f4 mv4 ttu tracked lh-title bt pv3\">Themes</h5>\n  {{ range first $number_tags site.Taxonomies.tags.ByCount }}\n      <a class=\"f6 link dim ba ph3 pv2 mb2 dib mr2\" href=\"{{ .Page.RelPermalink }}\">{{ .Page.Title }} ({{ .Count }})</a>\n  {{ end }}\n  </article>\n{{ end }}\n</section>\nFormspree is removing support for email-based forms, so my contact.md required a randomly-generated formspree_form_id: rather than an email address."
  },
  {
    "objectID": "blog/renovate/index.html#deployment",
    "href": "blog/renovate/index.html#deployment",
    "title": "A Fresh Lick of Paint",
    "section": "Deployment",
    "text": "Deployment\nManual\nInitially a few things did not render correctly, e.g. syntax highlighting, which it turned out required renaming the index.Rmd files to index.Rmarkdown. And when the manual deployment to Netlify got stuck uploading, I realised I also needed to change the .Rprofile to blogdown.method = 'markdown' rather than blogdown.method = 'html'.\nContinuous\nOnce the manual deployment to Netlify was working, I then moved on to continuous deployment via Github. I wanted to switch the commenting engine from Disqus to utterance.es and, as is often the case, wanting one thing results in the need for a bunch of other things; in this case, a public repo on Github. Installing the latter provides a more elegant fit with the Apéro design and has some nice advantages.\nAnd because I wanted to deploy a pre-existing RStudio project to Github, rather than following the usual Github-first practice, I found this guidance helpful.\nI played around a bit with the .gitignore file and found I could exclude quite a lot of stuff that Netlify would not need to do the Hugo build.\nThe Netlify deployment via Github did initially fail with a “Base directory does not exist” message. The fix there was to leave the base directory in Netlify’s build settings blank rather than using the repo URL (which it already had under current repository).\n\nThen finally I could flip my live site over to continuous deployment, pack away my paint pots, paint roller and step ladder, put my feet up in front of a roaring fire and bask in the warmth of my newly-renovated blogdown home.\nPost-deployment there was initially an issue with the RSS feed showing only the summary. Adding a layouts/_default/rss.xml file using the Hugo default with .Summary changed to .Content fixed that."
  },
  {
    "objectID": "blog/plunge/index.html",
    "href": "blog/plunge/index.html",
    "title": "Moving House",
    "section": "",
    "text": "After reading up on Blogdown, I decided to take the plunge and leave Wordpress for a quieter life in Blogdown."
  },
  {
    "objectID": "blog/plunge/index.html#motivation",
    "href": "blog/plunge/index.html#motivation",
    "title": "Moving House",
    "section": "Motivation",
    "text": "Motivation\nMy former site looked pretty good. But it was expensive to maintain.\nI was spending more than I wished to get a performant site. I could have spent less, and perhaps I’m easily seduced by “bells & whistles”, e.g. CloudFlare Plus and “GoGeek” hosting. But a non-speedy site is a bit of a turn-off.\nAnd it wasn’t just cost. It also took a lot of non-R effort to publish a post with Rmarkdown in the way I wanted. My main interest is in writing R code. Not wrestling Wordpress and multiple plugins into submission.\nA reboot was also a chance to re-brand. When I originally set up thinkr.biz I was initially unaware of a similarly-named site in France. Although my personal blog posed no threat across the Channel, and we co-existed for a few years, I anyway prefer having something a little more unique."
  },
  {
    "objectID": "blog/plunge/index.html#why-blogdown",
    "href": "blog/plunge/index.html#why-blogdown",
    "title": "Moving House",
    "section": "Why Blogdown?",
    "text": "Why Blogdown?\nI like Yihui Xie’s Blogdown primarily because it simplifies the path from Rmarkdown to blog. No more WWE-style detour. I can tweak a line of code in Rmarkdown, serve_site, and immediately see the updated blog locally. When I’m ready to publish, I just drag the public folder into Netlify, and voilà it’s live."
  },
  {
    "objectID": "blog/plunge/index.html#my-personal-roadmap",
    "href": "blog/plunge/index.html#my-personal-roadmap",
    "title": "Moving House",
    "section": "My personal roadmap",
    "text": "My personal roadmap\nThere are different routes one can take. Here’s mine.\n\nChoose a theme\nIn his book, Yihui advises asking yourself: “Do I like this fancy theme so much that I will definitely not change it in the next couple of years?” It’s very sound advice. Nonetheless, I’m easily seduced, so explored all possible fancy themes. In part because I like creating my own graphic art, so I wanted something that could help these little creations shine.\nOddly, I started by looking at one of Yihui’s recommended themes and discarded it, only to return to it again much later after an exhaustive exploration of other themes. There are many superficially nice Hugo themes. But when you actually play with them, there’s little below the surface and/or an absence of serious upkeep.\nHugo Academic is not the most appealing in the “shop window”. But when you take it for a test spin, and really take it through its paces, it offers a richness, flexibility and investment that reeled me in. After customising it to my taste, and paring back optional bits I do not need, simply by switching them off, it gave me something I feel very happy with.\n\n\nMigrate\nThere are assisted migration paths, e.g. from Wordpress, discussed in the Blogdown book. However I wanted to review and upgrade the R code in my original posts (only a dozen or so at the time). So, one-by-one, I copied each Rmd file into the projects folder of my new site, tweaked the code, and used serve_site to see the end product.\nI took this approach because R, especially the tidyverse and its ecosystem, is rapidly evolving. For example, the latest release of dplyr has some great new column-wise and row-wise functions. And spread and gather have been superseded by the more capable pivot_wider and pivot_longer. So it was a chance to upgrade my code.\nFor one or two of the more processing-intensive projects, I used either cache = TRUE in the code chunk, or saveRDS and readRDS to load data prepared earlier.\n\n\nBuild\nAn option I haven’t yet pursued is to host all my website source files in a GIT repository. Then Netlify could call Hugo to render my website automatically. Right now, my site content is simple enough to be able to use the Build Website button in RStudio.\nThe web-site is a static build, so it’s fast out-of-the-box, i.e. no need for speed-inducing wallet-slimming plugins.\n\n\nDeploy\nNetlify is recommended by bookdown.org. And it’s free for personal projects. The only small annual cost is my domain name.\nChoosing a domain name, which one can do via Netflify, is a little tricky. Many of the ideas one might have, have already occurred to someone else. And when you do find something available, there’s always that niggling feeling there may be something better out there.\nI chose Quantum Jitter for several reasons:\n\nI often use ggplot2’s geom_jitter\nLike a Quant, I have an interest in using machine learning to assess stock fundamentals\nLike the quantum world, my work features statistics and randomness\nIt was available\n\nAfter running the build in RStudio, which for my site only takes a few minutes, I can simply drag my newly-created public folder into Netlify’s Deploys page and bingo, the site’s live in a jiffy.\nSo, if you are toying with the idea of moving house, I can recommend a quieter life in Blogdown."
  },
  {
    "objectID": "project/forecast/index.html",
    "href": "project/forecast/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Humans have the magical ability to plan for future events, for future gain. It’s not quite a uniquely human trait. Because apparently ravens can match a four-year-old.\nAn abundance of data, and some very nice R packages, make our ability to plan all the more powerful.\nIn the Spring of 2018 I looked at sales from an historical perspective in Six Months Later.. Here I’ll use the data to model a time-series forecast for the year ahead. The techniques apply to any time series with characteristics of trend, seasonality or longer-term cycles.\nWhy forecast sales? Business plans require a budget, e.g. for resources, marketing and office space. A good projection of revenue provides the foundation for the budget. And, for an established business, with historical data, time-series forecasting is one way to deliver a robust projection.\nWithout exogenous data, the forecast assumes one continues to do what one’s doing. So, it provides a good starting-point. Then one might, for example, add assumptions about new products or services. And, if there is forward-looking data available, for example, market size projections (with past projections to train the model), then one could feed this into the forecast modelling too.\nFirst I’ll check the encoding of the data.\nNext I’ll set up a vector of column names to apply consistently to both files, and import the data with the suggested encoding.\nI’d like to create some new features: Month-end dates, something to distinguish between the two frameworks (G-Cloud or DOS). The spend has a messy format and needs a bit of cleaning too.\nThe lot structure for G-Cloud has evolved over time, but fortunately, there is a simple mapping, i.e. PaaS and IaaS became Cloud Hosting, SaaS became Cloud Software, and Specialist Cloud Services became Cloud Support, so I’ll standardise on the latter.\nThe tidied data now needs to be converted to a tsibble(Wang, Cook, and Hyndman 2020), the temporal equivalent of a tibble(Müller and Wickham 2022).\nR has evolved since I first wrote this post. At that time, it was necessary to either split the data into the two frameworks (G-Cloud and DOS) and forecast them separately. Or, as I did with the three G-Cloud lots, use the purrr package to iterate through a forecast.\nThe tsibble package combined with the newer fable(O’Hara-Wild, Hyndman, and Wang 2022a) and feasts(O’Hara-Wild, Hyndman, and Wang 2022b) packages, make this easier. One of the defining feature of the tsibble is the key. I want a model for each framework, so I’m setting this as the tsibble key (and the temporal variable as the tsibble index).\nBy decomposing the historical data we can tease out the underlying trend and seasonality:\nI’ll use auto.arima: AutoRegressive Integrated Moving Average modelling which aims to describe the autocorrelations in the data.\nBy setting stepwise and approximation to FALSE, auto.arima will explore a wider range of potential models.\nI’ll forecast with the default 80% and 95% prediction intervals. This means the darker-shaded 80% range should include the future sales value with an 80% probability. Likewise with a 95% probability when adding the wider and lighter-shaded area.\nUse of autoplot would simplify the code, but personally I like to expose all the data, for example unpacking the prediction intervals, and have finer control over the visualisation.\nThe G-Cloud framework compromises three lots: Cloud Hosting, Cloud Software and Cloud Support.\nI previously combined auto.arima (from the forecast package) with functions from the sweep package, to create multiple forecasts in one shot. tsibble coupled fabletools handle this with the key set to the lot variable.\nAn alternative option is hierarchical time-series forecasting which models bottom-up, top-down or middle-out, and ensures the sum of the forecasts at the lower level sum to the top-level forecast. This approach has pros and cons and is not considered here.\nSo ravens are not yet ready for forecasting with R. But then neither are 4-year-olds, are they?"
  },
  {
    "objectID": "project/forecast/index.html#r-toolbox",
    "href": "project/forecast/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[9]; is.na[1]; library[5]; mean[2]; print[1]; sum[2]\n\n\nclock\nadd_days[1]; add_months[1]; date_parse[1]\n\n\ndplyr\nfilter[1]; bind_rows[3]; if_else[1]; mutate[4]; recode[1]; rename[2]; select[2]; summarise[2]\n\n\nfable\nARIMA[2]\n\n\nfabletools\ncomponents[1]; forecast[2]; glance[2]; hilo[4]; model[3]; tidy[2]; unpack_hilo[2]\n\n\nfeasts\nSTL[1]\n\n\nggplot2\naes[11]; element_text[2]; facet_wrap[2]; geom_line[5]; geom_ribbon[4]; ggplot[3]; labs[4]; scale_colour_manual[2]; scale_y_continuous[3]; theme[2]; theme_bw[1]; theme_set[1]\n\n\npurrr\nmap[1]; set_names[1]; walk[1]\n\n\nreadr\nguess_encoding[1]; locale[1]; parse_number[1]; read_csv[1]\n\n\nscales\nlabel_dollar[3]\n\n\nstringr\ncoll[1]; str_c[3]; str_extract[1]; str_remove[1]; str_replace[1]\n\n\ntsibble\nyearmonth[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/wood/index.html",
    "href": "project/wood/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "In Criminal Goings-on faceting offered a way to get a sense of the data. This is a great visualisation tool building on the principle of small multiples. There may come a point though where the sheer volume of small multiples make it harder to “see the wood for the trees”. What’s an alternative strategy?\nThis time I’ll use Van Gogh’s “The Starry Night” palette for the feature image and plots. And there are 9 types of criminal offence, so colorRampPalette will enable the interpolation of an extended set.\nThe data need a little tidy-up.\nThis was the original visualisation in Criminal Goings-on using ggplot’s facet_wrap.\nThere are some nice alternatives which allow one to go deeper into the data whilst making the whole experience more consumable and engaging.\nSwitching facet_wrap for facet_trelliscope is a simple option. Or trelliscope(Hafen and Schloerke 2021) may be used in combination with the rbokeh (Hafen 2021) (or plotly) packages. Irrespective of the option chosen, one can more flexibly display the several hundred “small multiple” panels required to go deeper into the crime data.\nPairing trelliscope with rbokeh permits the addition of some custom cognostics and additional interactivity. The slope cognostic, for example, enables filtering on the boroughs and types of offence exhibiting the steepest upward or downward trends."
  },
  {
    "objectID": "project/wood/index.html#r-toolbox",
    "href": "project/wood/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.numeric[1]; c[4]; library[5]; list[2]; mean[1]; numeric[1]; round[1]; sum[1]\n\n\ndplyr\nfilter[1]; mutate[3]; summarise[1]\n\n\nggplot2\naes[1]; element_rect[1]; element_text[1]; facet_wrap[1]; geom_line[1]; ggplot[1]; guide_legend[1]; guides[1]; labs[1]; scale_colour_manual[1]; theme[1]; theme_bw[1]; theme_set[1]\n\n\ngrDevices\ncolorRampPalette[1]\n\n\njanitor\nclean_names[1]\n\n\nrbokeh\nfigure[1]; ly_lines[1]; ly_points[1]; theme_plot[1]\n\n\nreadr\nread_csv[1]\n\n\nstats\nIQR[1]; coef[1]; lm[1]\n\n\nstringr\nstr_c[1]; str_extract[1]; str_wrap[1]\n\n\ntibble\ntibble[1]\n\n\ntidyr\nnest[1]\n\n\ntrelliscopejs\ncog[3]; map_cog[1]; map_plot[1]; sort_spec[1]; trelliscope[1]\n\n\nvangogh\nvangogh_palette[1]"
  },
  {
    "objectID": "project/hansard/index.html",
    "href": "project/hansard/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Before each vote, the Speaker of the House yells “Division! Clear the Lobby”. I’d like to find which cluster of MPs (Members of Parliament) may be exiting the lobby and going their own way.\nHansard reports what’s said in the UK Parliament, sets out details of divisions, and records decisions taken during a sitting. The R package hansard package (Odell 2017) provides access to the data.\nI’ll start by building a list of all Labour Party MPs.\nCreating a function will enable me to iterate through the MP list to extract their voting records.\nI’ll use it to extract the “aye” and “no” votes. Use of possibly prevents the code from stopping when it encounters former MPs for whom no data is returned.\nVoting the opposite way to the majority of the party, as well as non-votes, will both be of interest when assessing which MPs are “most distant” from the wider party.\nThe data are standardised (i.e. scaled) to ensure comparability. This is verified by ensuring the mean and standard deviation are close to zero and one respectively.\nI’d like to assess whether the data contain meaningful clusters rather than random noise. This is achieved quantitatively by calculating the Hopkins statistic, and visually by inspection.\nIf the Hopkins statistic is closer to 1 than 0, then we have data which may be clustered.\nA visual assessment of clustering tendency reveals distance data exhibiting a visible structure.\nThere are eight methods I could use for hierarchical clustering, and I’ll need to determine which will yield results that best fit the data.\nThe correlation plot below shows that the median and ward methods have a weaker correlation with the other five methods.\nThe above plot does not tell us which method is optimal. For that, I’ll take each of the cluster agglomeration methods and calculate their cophenetic distances. I can then correlate these with the original distance to see which offers the best fit.\nThe plot below confirms the ward and median methods having a weaker fit. Average produces the strongest correlation coefficient of 0.98.\nI can now plot the full Labour Party dendrogram using the average method. This shows a “cluster of six” MPs which is the last to merge with the rest of the party based on their voting pattern.\nI’ll zoom in on the “cluster of six”.\nSummarising and sorting the total votes by MP tells me that the “cluster of six” MPs are among the eight MPs voting the fewest times. And I can, for example, verify the record for Emma Reynolds directly via Hansard.\nNon-voting will not be the only influencing factor. The “distant cluster” will be particularly influenced by a small minority of MPs voting in the opposite direction to the overwhelming majority.\nCook’s Distance visualises these influential outliers. This shows the voting of three MPs, all on the European Union Withdrawal Bill readings, to be particular outliers. All three MPs are in the “cluster of six”."
  },
  {
    "objectID": "project/hansard/index.html#r-toolbox",
    "href": "project/hansard/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[7]; cut[1]; labels[1]; library[10]; list[3]; max[2]; min[2]; nrow[1]; readRDS[2]; rev[2]; round[1]; saveRDS[2]; scale[1]\n\n\nbroom\naugment[1]\n\n\nclock\ndate_format[2]; date_parse[2]\n\n\ncorrplot\ncorrplot[2]\n\n\ndendextend\nassign_values_to_leaves_nodePar[3]; color_branches[2]; cor.dendlist[1]; dendlist[2]; set[3]\n\n\ndplyr\nfilter[3]; across[2]; c_across[4]; if_else[3]; left_join[2]; mutate[5]; n[1]; rename[2]; select[5]; slice_min[1]; summarise[2]; transmute[1]\n\n\nfactoextra\nfviz_dist[1]; get_clust_tendency[1]\n\n\nforcats\nfct_reorder[1]\n\n\nggplot2\naes[8]; coord_flip[4]; element_blank[3]; element_line[1]; element_text[2]; geom_col[2]; geom_jitter[1]; geom_label[1]; geom_point[1]; geom_text[1]; ggplot[6]; ggtitle[1]; labs[5]; position_stack[1]; scale_colour_manual[2]; scale_fill_manual[1]; theme[4]; theme_bw[1]; theme_set[1]; theme_void[1]; unit[1]\n\n\nggrepel\ngeom_label_repel[2]\n\n\nglue\nglue[2]\n\n\nhansard\ncommons_members[1]; mp_vote_record[1]\n\n\npurrr\ncompact[1]; list_rbind[2]; map[4]; pluck[1]; possibly[1]; reduce[1]; set_names[1]\n\n\nstats\nas.dendrogram[2]; cophenetic[1]; cor[1]; dist[2]; hclust[3]; lm[1]; order.dendrogram[1]; reorder[1]; sd[1]\n\n\nstringr\nstr_c[4]; str_detect[1]; str_replace[2]; str_wrap[1]\n\n\ntibble\nas_tibble[1]; tibble[1]\n\n\ntidyr\npivot_longer[2]; pivot_wider[2]\n\n\nwesanderson\nwes_palette[3]"
  },
  {
    "objectID": "project/forest/index.html",
    "href": "project/forest/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "When first posted in 2018 this project used the caret package to model crime in London. Since then, the newer tidymodels(Kuhn and Wickham 2020) framework, consistent with tidy data principles, has rapidly evolved.\nThis custom palette was created in Adobe Colour as the basis for the feature image above and with the hex codes loaded for use in ggplot. colorRampPalette enables interpolation of an extended set of colours to support the number of offence types.\nA faceted plot is one way to get a sense of the data.\nVisualising data in small multiples using facet_wrap or facet_grid can be a useful way to explore data. When there are a larger number of these however, as we’re starting to see in the example above, there are alternative techniques one can employ. This is explored in Seeing the Wood for the Trees.\nNonetheless, one can anyway see there are data aggregated at multiple levels. So to net these data down to purely borough-level, I’ll filter out the summarised rows, for example, “England and Wales” and “Inner London”.\nThere are 9 types of offence in 33 boroughs. The dataset covers the period 1999 to 2016.\nThe faceted plot hints at a potential interaction between borough and type of offence. In more affluent boroughs, and/or those attracting greater visitor numbers, e.g. Westminster and Kensington & Chelsea, “theft and handling” is the more dominant category. In Lewisham, for example, “violence against the person” exhibits higher counts. However, for the purpose of this basic model comparison, I’m going to set aside the potential interaction term.\nBefore modelling, I’ll visualise the dependent variable against each independent variable.\nThe offences and borough variables show significant variation in crime counts. And there is also evidence of a change over time.\nI’ll separate out some test data so I can compare the performance of the models on data they have not see during model training.\nI’m using the recipes package to establish the role of the variables. Alternatively I could have used a formula-based approach, i.e. number_of_offences ~ borough + offences + year.\nWhilst borough and offences are nominal, I’m not creating any dummy variables since I intend to use tree-based models which will anyway branch left and right based on groups of values.\nI’ll start with a Recursive Partitioning And Regression Trees (rpart) model. The feature importance plot tells me which variables are having the biggest influence on the model. The type of offence is the most important predictor in the rpart model, followed by the location of the offences. This makes intuitive sense.\nClearly there is a temporal component too otherwise there would be no trend.\nRanger is an implementation of random forests or recursive partitioning that, according to the documentation, is particularly suited to high dimensional data. My data is not high-dimensional, but let’s throw it into the mix.\nAnd of course my project title would make little sense without a Random Forest.\nFor good measure, I’ll also include a generalized linear model (glm)\nThe Random Forest and the glm models performed the best here, with the former edging the Mean Absolute Error and R Squared metrics, and the latter with its nose in front on the Root Mean Squared Error.\nAnother way of approaching all this would be to use time-series forecasting. This would major on auto-regression, i.e. looking at how the lagged number-of-offences influence future values. And one could further include exogenous data such as, say, the numbers of police. It would be reasonable to expect that increasing police numbers would, in time, lead to decreased crime levels.\nI explored time-series in other posts such as Digging Deep, so I won’t go down that path here.\nWhat I could do though is to strengthen my tree-based models above by engineering some additional temporal features. Let’s try that just with the Random Forest to see if it improves the outcome.\nSo, when predicting the number of offences, the model will now additionally consider, for each borough, type of offence and year, the number of offences in each of the three prior years.\nThe recipe summary includes the three new predictors. And the feature importance plot shows the lags playing a larger role in the model than the year variable, so looks like we should anticipate a model improvement.\nThe model metrics bear this out. The mae and rmse are markedly smaller, and the rsq significantly improved. We could have tried further lags. We could have tried tweaking some parameters. We could have tried time-series forecasting with, for example a statistical model like ARIMA, or a Neural Network model such as NNETAR.\nThe best approach would depend upon a more precise definition of the objective. And some trial and error, comparing approaches after more extensive feature-engineering, validation, testing and tuning. For the purposes of this post though I wanted to merely explore some techniques. So I’ll leave it there."
  },
  {
    "objectID": "project/forest/index.html#r-toolbox",
    "href": "project/forest/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[3]; as.numeric[1]; c[7]; character[3]; library[6]; list[5]; numeric[1]; round[2]; set.seed[2]; sum[4]; summary[2]\n\n\ndplyr\nfilter[1]; bind_rows[4]; group_by[1]; mutate[10]; summarise[4]\n\n\nforcats\nfct_inorder[1]; fct_reorder[2]\n\n\nggplot2\naes[10]; annotate[1]; coord_flip[1]; element_text[3]; facet_wrap[3]; geom_boxplot[2]; geom_col[3]; geom_label[3]; geom_line[2]; geom_smooth[1]; ggplot[7]; guide_legend[1]; guides[1]; labs[11]; scale_colour_manual[1]; scale_fill_manual[3]; scale_y_continuous[1]; scale_y_log10[2]; theme[4]; theme_bw[1]; theme_set[1]; theme_void[1]\n\n\ngrDevices\ncolorRampPalette[1]\n\n\njanitor\nclean_names[1]\n\n\nparsnip\ndecision_tree[1]; poisson_reg[1]; rand_forest[3]; set_engine[5]; set_mode[5]\n\n\nreadr\nread_csv[1]\n\n\nrecipes\nhas_role[2]; recipe[2]; update_role[4]\n\n\nrsample\ninitial_split[2]; testing[2]; training[2]\n\n\nscales\ncut_short_scale[3]; label_number[3]\n\n\nstats\nmedian[2]\n\n\nstringr\nstr_c[1]; str_extract[1]; str_wrap[3]\n\n\ntibble\ntibble[1]\n\n\ntidyr\ndrop_na[1]\n\n\nvip\nvip[6]\n\n\nworkflows\nadd_model[5]; add_recipe[5]; workflow[5]\n\n\nyardstick\nmetrics[2]"
  },
  {
    "objectID": "project/un/index.html",
    "href": "project/un/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "In Finding Happiness in ‘The Smoke’, dimension reduction and cluster analysis are used to see how different characteristics group London boroughs.\nDimension reduction is used here to visualise the grouping of UN members, for example five of the founding members, based on their General Assembly voting patterns. And by using animation, it’s possible to more easily see changes over time. Are they drifting closer or farther apart?\nGeneral Assembly votes are obtained from unvotes(Robinson 2021) and the visualisation animated using gganimate(Pedersen and Robinson 2022).\nApplying a sliding window to the roll-calls from 1946 to 2019 will make it possible to show the temporal changes.\nDimensionality reduction may be performed on each window. And the voting patterns are then visualised as a two-dimensional animation.\nFrance and the UK, for example, have remained particularly close given their historical ties and geographical proximity.\nThe UN’s Security Council Veto List provides further insights on the changing profile of P5 voting over the decades."
  },
  {
    "objectID": "project/un/index.html#r-toolbox",
    "href": "project/un/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[2]; c[11]; character[2]; format[1]; library[10]; max[3]; min[1]; seq[1]\n\n\nclock\ndate_build[1]; date_parse[2]; get_year[5]\n\n\ndplyr\nfilter[4]; arrange[2]; count[2]; group_by[1]; if_else[7]; inner_join[1]; mutate[8]; n[2]; n_distinct[1]; pull[3]; recode[1]; row_number[1]; select[5]; slice[1]; summarise[3]; ungroup[1]\n\n\nforcats\nfct_reorder[1]\n\n\ngganimate\nanimate[1]; gganimate[1]; shadow_wake[1]; transition_time[1]\n\n\nggplot2\naes[5]; coord_flip[1]; element_text[1]; geom_col[2]; geom_label[2]; ggplot[3]; labs[3]; scale_fill_manual[3]; scale_x_continuous[1]; theme[1]; theme_bw[1]; theme_set[1]\n\n\nglue\nglue[3]\n\n\npatchwork\nplot_layout[1]\n\n\npurrr\nlist_rbind[1]; map[1]\n\n\nrvest\nhtml_element[1]; html_table[1]; read_html[1]\n\n\nstats\nprcomp[1]\n\n\nstringr\nstr_detect[5]; str_remove[1]; str_replace[1]\n\n\ntibble\nas_tibble[1]\n\n\ntidyr\ncomplete[1]; fill[1]; nest[2]; nesting[1]; pivot_longer[1]; pivot_wider[1]; replace_na[1]; unnest[2]\n\n\ntsibble\nas_tsibble[1]; slide_tsibble[1]; tsibble[1]\n\n\nutils\ndata[2]\n\n\nwesanderson\nwes_palette[2]"
  },
  {
    "objectID": "project/cetacea/index.html",
    "href": "project/cetacea/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "The Natural History Museum began recording cetacean (whales, dolphins and porpoises) strandings in 1913 (Natural History Museum 2019). Let’s explore this 1913-1989 dataset."
  },
  {
    "objectID": "project/cetacea/index.html#exploratory",
    "href": "project/cetacea/index.html#exploratory",
    "title": "Toolbox",
    "section": "Exploratory",
    "text": "Exploratory\nSome of the species labels contain a question mark or forward slash. This indicates uncertainty, so it might be fun to see if a machine learning model (multi-class classification) could learn from the known species and suggest an appropriate species where it’s uncertain.\n\nstrandings_df2 <- \n  strandings_df |> \n  mutate(species_uncertainty =\n      if_else(str_detect(species, \"[?/]\"), \"Uncertain\", \"Known\"))\n\nstrandings_df2 |> \n  filter(species_uncertainty == \"Uncertain\") |> \n  count(species, sort = TRUE, name = \"Count\") |> \n  slice_head(n = 6)\n\n\n\n\nspecies\nCount\n\n\n\ndelphis/coeruleoalba\n48\n\n\nphocoena?\n42\n\n\nmelaena?\n20\n\n\ndelphis?\n18\n\n\ntruncatus?\n18\n\n\nacutorostrata?\n12\n\n\n\n\n\n\nThe date variable has many NA’s. Fortunately, the components to construct many of these are in the year_val, month_val and day_val variables. With a little wrangling and imputation, we can coalesce these variables into a new date. This will be useful since plots of sample species by year, month and week of stranding suggest a de-constructed date could be a useful predictor.\n\nstrandings_df2 |> \n  select(date_rep, year_val, month_val, day_val) |> \n  summary()\n\n    date_rep             year_val      month_val         day_val     \n Min.   :1913-02-13   Min.   :   0   Min.   : 0.000   Min.   : 0.00  \n 1st Qu.:1933-09-09   1st Qu.:1933   1st Qu.: 4.000   1st Qu.: 9.00  \n Median :1954-04-13   Median :1955   Median : 7.000   Median :16.00  \n Mean   :1955-01-08   Mean   :1954   Mean   : 6.766   Mean   :15.66  \n 3rd Qu.:1979-03-21   3rd Qu.:1979   3rd Qu.:10.000   3rd Qu.:22.00  \n Max.   :1989-12-25   Max.   :1989   Max.   :12.000   Max.   :31.00  \n NA's   :121                                                         \n\nstrandings_df3 <- strandings_df2 |>\n  mutate(\n    month_val = if_else(month_val == 0, mean(month_val) |> \n                          as.integer(), month_val),\n    day_val = if_else(day_val == 0, mean(day_val) |> as.integer(), day_val),\n    day_val = if_else(day_val == 0, 1L, day_val),\n    date2 = date_build(year_val, month_val, day_val, invalid = \"NA\"),\n    .by = species\n  ) |> \n  mutate(date3 = coalesce(date_rep, date2),\n         date_rep = if_else(is.na(date_rep), lag(date3), date3)\n         ) |> \n  select(-date2, -date3, -ends_with(\"_val\"))\n\nexample_species <-\n  c(\"musculus\", \"melas\", \"crassidens\", \"borealis\", \"coeruleoalba\")\n\nknown_species <- strandings_df3 |> \n  filter(species_uncertainty == \"Known\")\n\nplot_date_feature <- \\(var) {\n  known_species |>\n    mutate(\n      year = get_year(date_rep),\n      month = get_month(date_rep),\n      week = as_iso_year_week_day(date_rep) |> get_week()\n    ) |>\n    filter(species %in% example_species) |>\n    count(species, {{ var }}) |>\n    ggplot(aes(species, {{ var }})) +\n    geom_violin(\n      alpha = 0.7,\n      fill = cols[3],\n      show.legend = FALSE\n    ) +\n    coord_flip() +\n    labs(\n      title = glue(\"Variation in {str_to_title(as.character(var))}\",\n                   \" of Stranding for Known Species\"),\n      x = NULL, y = glue(\"{str_to_title(as.character(var))}\")\n    )\n}\n\nc(\"year\", \"month\", \"week\") |> \n  map(sym) |> \n  map(plot_date_feature) |> \n  wrap_plots(ncol = 1)\n\n\n\n\nDo latitude and longitude carry useful predictive information?\nA geospatial visualisation of strandings shows some species do gravitate towards particular stretches of coastline, e.g. “acutus” and “albirostris” to the east, and “coeruleoalba” to the south-west.\nSome species may also be more prone to mass stranding, so something that indicates whether a species has such a history (in these data) may be worth including in the mix.\n\nuki <- map_data(\"world\", region = c(\"uk\", \"ireland\"))\n\nlabels <- c(\"Mass\", \"Single\")\n\nuki |> \n  ggplot() +\n  geom_map(aes(long, lat, map_id = region), map = uki, \n           colour = \"black\", fill = \"grey90\", size = 0.1) +\n  geom_jitter(aes(longitude, latitude, colour = mass_single, \n                  size = mass_single), \n              alpha = 0.5, data = known_species) +\n  facet_wrap(~ species_lumped, nrow = 3) +\n  coord_map(\"mollweide\") +\n  scale_size_manual(values = c(1, 0.5), labels = labels) +\n  scale_colour_manual(values = cols[c(3, 2)], labels = labels) +\n  theme_void() +\n  theme(legend.position = \"top\", \n        strip.text = element_text(colour = \"grey50\")) +\n  labs(title = \"Strandings by Species\", \n       colour = NULL, size = NULL)\n\n\n\n# Add history of mass stranding\nstrandings_df4 <- strandings_df3 |> \n  mutate(mass_possible = min(mass_single, na.rm = TRUE),\n         .by = species)\n\nSome records are missing the length measurement of the mammal. Nonetheless, where present, this is likely to be predictive, and may help, for example, separate species labelled as “delphis/coeruleoalba” where the length is at the extreme ends of the “delphis” range as we see below. And the range of length may differ by mammal sex.\n\nknown_species |>\n  mutate(sex = case_when(\n    sex == \"M\" ~ \"Male\",\n    sex == \"F\" ~ \"Female\",\n    TRUE       ~ \"Unknown\"\n  )) |> \n  filter(species_lumped != \"Other\") |> \n  count(species_lumped, length, sex) |> \n  mutate(species_lumped = fct_reorder(species_lumped, \n                                      desc(length), min, na.rm = TRUE)) |> \n  ggplot(aes(species_lumped, length)) + \n  geom_violin(aes(fill = if_else(str_detect(species_lumped, \"^de|^co\"), \n                                 TRUE, FALSE)), show.legend = FALSE) +\n  facet_wrap(~ sex) +\n  scale_fill_manual(values = cols[c(1, 5)]) +\n  coord_flip() +\n  labs(title = \"Variation in Species Length by Sex\", \n       x = NULL, y = \"Length (metres)\")\n\n\n\n\nWith map coordinates not always available, county could be, with a little string cleaning, a useful additional predictor.\n\nstrandings_df4 |> \n  count(county) |> \n  filter(str_detect(county, \"Shet|Northumberland\")) |> \n  rename(County = county, Count = n)\n\n\n\n\nCounty\nCount\n\n\n\nFair Isle, Shetland Isles\n1\n\n\nNorthumberland\n89\n\n\nNorthumberland.\n1\n\n\nShetland Islands, Scotland\n232\n\n\nShetland Isles, Scotland\n35\n\n\nShetland, Scotland\n1\n\n\nShetlands, Scotland\n1\n\n\n\n\n\nregex_pattern <-\n  c(\"[,/].*$\",\n    \"(?<!Che|Hamp|Lanca|North York)-?shire\",\n    \" Isl.*$\",\n    \" &.*$\",\n    \"[-.]$\")\n\nstrandings_df5 <- strandings_df4 |>\n  mutate(\n    county = str_remove_all(county, str_c(regex_pattern, collapse = \"|\")),\n    county = recode(\n      county,\n      \"Carnarvon\" = \"Caernarvon\",\n      \"E.Lothian\" = \"East Lothian\",\n      \"Shetlands\" = \"Shetland\",\n      \"W.Glamorgan\" = \"West Glamorgan\",\n      \"S.Glamorgan\" = \"South Glamorgan\"\n    )\n  ) \n\nstrandings_df4 |>\n  summarise(counties_before = n_distinct(county))\n\n\n\n\ncounties_before\n\n\n146\n\n\n\n\nstrandings_df5 |>\n  summarise(counties_after = n_distinct(county))\n\n\n\n\ncounties_after\n\n\n109\n\n\n\n\n\nWhilst count also appears to hold, based on the plot pattern below, species-related information, I’m not going to use it as a predictor as we do not know enough about how it was derived, as reflected in these variable descriptions.\n\nstrandings_df5 |>\n  ggplot(aes(species, count, colour = species_uncertainty)) +\n  geom_jitter(alpha = 0.5, size = 2) +\n  coord_flip() +\n  scale_y_log10() +\n  scale_colour_manual(values = cols[c(1, 5)]) +\n  labs(title = \"How 'Count' Relates to Species\", \n       x = NULL, y = \"Count (log10)\", colour = \"Species\") +\n  theme(legend.position = \"top\")"
  },
  {
    "objectID": "project/cetacea/index.html#modelling",
    "href": "project/cetacea/index.html#modelling",
    "title": "Toolbox",
    "section": "Modelling",
    "text": "Modelling\nSo, I’ll set aside the rows where the species is uncertain (to be used later for new predictions), and I’ll train a model on 75% of known species, and test it on the remaining 25%. I’ll use the following predictors:\n\n\nlatitude and longitude\n\nMammal length and sex\n\n\nmass_possible indicating a species history of mass strandings\n\ndate reported converted into decimal, week, month and year\n\ncounty may be useful, especially where the longitude and latitude are missing\n\nfam_genus which narrows the range of likely species\n\nI’d like to also make use of the textrecipes(Hvitfeldt 2022) package. I can tokenise the textual information in rep_comment and location to see if these add to the predictive power of the model.\nI’ll tune the model using tune_race_anova(Kuhn 2022) which quickly discards hyperparameter combinations showing little early promise.\n\nknown_species <- strandings_df5 |>\n  filter(species_uncertainty == \"Known\") |>\n  mutate(across(\n    c(\n      \"species\",\n      \"mass_single\",\n      \"mass_possible\",\n      \"county\",\n      \"location\",\n      \"sex\",\n      \"fam_genus\"\n    ),\n    factor\n  ))\n\nset.seed(123)\n\ndata_split <-\n  known_species |>\n  mutate(species = fct_drop(species)) |> \n  initial_split(strata = species)\n\ntrain <- data_split |> training()\n\ntest <- data_split |> testing()\n\npredictors <-\n  c(\n    \"latitude\",\n    \"longitude\",\n    \"length\",\n    \"mass_single\",\n    \"mass_possible\",\n    \"county\",\n    \"location\",\n    \"rep_comment\",\n    \"sex\",\n    \"fam_genus\"\n  )\n\nrecipe <-\n  train |>\n  recipe() |>\n  update_role(species, new_role = \"outcome\") |>\n  update_role(all_of(predictors), new_role = \"predictor\") |>\n  update_role(!has_role(\"outcome\") & !has_role(\"predictor\"), \n              new_role = \"id\") |>\n  step_date(date_rep, features = c(\"decimal\", \"week\", \"month\", \"year\"), \n            label = FALSE) |>\n  step_tokenize(location, rep_comment) |>\n  step_stopwords(location, rep_comment) |>\n  step_tokenfilter(location, rep_comment, max_tokens = tune()) |> #100\n  step_tf(location, rep_comment) |>\n  step_zv(all_predictors()) |>\n  step_dummy(all_nominal_predictors())\n\nxgb_model <-\n  boost_tree(trees = tune(), # 440\n             mtry = tune(), # 0.6\n             learn_rate = 0.02) |> \n  set_mode(\"classification\") |>\n  set_engine(\"xgboost\", \n             count = FALSE,\n             verbosity = 0,\n             tree_method = \"hist\")\n\nxgb_wflow <- workflow() |>\n  add_recipe(recipe) |>\n  add_model(xgb_model)\n\nset.seed(9)\n\nfolds <- vfold_cv(train, strata = species)\n\nset.seed(10)\n\ntic()\n\ntune_result <- xgb_wflow |>\n  tune_race_anova(\n    resamples = folds,\n    grid = crossing(\n      trees = seq(200, 520, 40),\n      mtry = seq(0.5, 0.7, 0.1),\n      max_tokens = seq(80, 120, 20)\n      ),\n    control = control_race(),\n    metrics = metric_set(accuracy)\n  )\n\ntoc()\n\n396.407 sec elapsed\n\ntune_result |> \n  plot_race() + \n  labs(title = \"Early Elimination of Parameter Combinations\")\n\n\n\nset.seed(123)\n\nxgb_fit <- xgb_wflow |>\n  finalize_workflow(tune_result |> \n                      select_best(metric = \"accuracy\")) |> \n  fit(train)\n\nHaving fitted the model with the 3080 records in the training data, I’ll test its accuracy on the 1028 records of known species the model has not yet seen.\nWithout spending time on alternative models, we’re getting a reasonable result for the “porpoise” of this post, as reflected in both the accuracy metric and confusion matrix.\n\nxgb_results <- xgb_fit |> \n  augment(new_data = test)\n\nxgb_results |>\n  accuracy(species, .pred_class)\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\naccuracy\nmulticlass\n0.9931907\n\n\n\n\nxgb_results |>\n  conf_mat(species, .pred_class) |>\n  autoplot(type = \"heatmap\") +\n  scale_fill_gradient2(\n    mid = \"white\",\n    high = cols[1],\n    midpoint = 0\n  ) +\n  labs(title = \"Confusion Matrix\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nThe top variable importance scores include fam_genus, many of the rep_comment tokens, plus length, mass-possible, date_decimal, date_year, and latitude.\n\nvi(xgb_fit |> extract_fit_parsnip()) |> \n  arrange(desc(Importance)) |> \n  mutate(ranking = row_number()) |> \n  slice_head(n = 40)\n\n\n\n\nVariable\nImportance\nranking\n\n\n\nfam_genus_Phocoena\n0.1215845\n1\n\n\nfam_genus_Globicephala\n0.0816541\n2\n\n\ntf_rep_comment_unidentified\n0.0736307\n3\n\n\nfam_genus_Delphinus\n0.0710860\n4\n\n\nfam_genus_Tursiops\n0.0500141\n5\n\n\ntf_rep_comment_false\n0.0498405\n6\n\n\nfam_genus_Lagenorhynchus\n0.0448997\n7\n\n\ntf_rep_comment_finned\n0.0341306\n8\n\n\ntf_rep_comment_sided\n0.0302409\n9\n\n\ntf_rep_comment_long\n0.0244866\n10\n\n\nfam_genus_Hyperoodon\n0.0240353\n11\n\n\nlength\n0.0230962\n12\n\n\nfam_genus_Grampus\n0.0227513\n13\n\n\ntf_rep_comment_beaked\n0.0219021\n14\n\n\ntf_rep_comment_lesser\n0.0215853\n15\n\n\ntf_rep_comment_rorqual\n0.0199182\n16\n\n\ntf_rep_comment_dolphin\n0.0198597\n17\n\n\nfam_genus_Orcinus\n0.0194480\n18\n\n\ntf_rep_comment_porpoise\n0.0192418\n19\n\n\ntf_rep_comment_bottle\n0.0165892\n20\n\n\nfam_genus_Pseudorca\n0.0159957\n21\n\n\nfam_genus_Physeter\n0.0159090\n22\n\n\ntf_rep_comment_risso’s\n0.0131135\n23\n\n\nmass_possible_S\n0.0127573\n24\n\n\nfam_genus_Mesoplodon\n0.0123743\n25\n\n\ntf_rep_comment_fin\n0.0122737\n26\n\n\nfam_genus_Ziphius\n0.0122521\n27\n\n\nfam_genus_odontocete\n0.0109274\n28\n\n\nfam_genus_cetacean\n0.0104412\n29\n\n\ntf_rep_comment_killer\n0.0099741\n30\n\n\nfam_genus_Stenella\n0.0087018\n31\n\n\ndate_rep_decimal\n0.0081614\n32\n\n\ntf_rep_comment_nosed\n0.0075943\n33\n\n\ndate_rep_year\n0.0072361\n34\n\n\ntf_rep_comment_whale\n0.0071684\n35\n\n\ntf_rep_comment_white\n0.0067214\n36\n\n\nmass_single_S\n0.0041988\n37\n\n\ntf_rep_comment_common\n0.0041767\n38\n\n\ntf_rep_comment_cuvier’s\n0.0036603\n39\n\n\ntf_rep_comment_sowerby’s\n0.0036372\n40\n\n\n\n\n\n\nDo the predictions look reasonable?\nThe class probability is spread across 27 species. I’m going to set a high threshold of 0.9, meaning the predicted species needs to be a pretty confident prediction.\n\nxgb_preds <- xgb_fit |> \n  augment(new_data = strandings_df5 |> \n            filter(species_uncertainty == \"Uncertain\"))\n\nspecies_levels <- xgb_preds |> \n  select(starts_with(\".pred_\"), -.pred_class) |> \n  names() |> \n  as.factor()\n\nsubset_df <- xgb_preds |>\n  mutate(\n    .class_pred = make_class_pred(\n      .pred_acutorostrata,\n      .pred_acutus,\n      .pred_albirostris,\n      .pred_ampullatus,\n      .pred_bidens,\n      .pred_borealis,\n      .pred_breviceps,\n      .pred_cavirostris,\n      .pred_coeruleoalba,\n      .pred_crassidens,\n      .pred_delphis,\n      .pred_electra,\n      .pred_europaeus,\n      .pred_griseus,\n      .pred_leucas,\n      .pred_macrocephalus,\n      .pred_melaena,\n      .pred_melas,\n      .pred_mirus,\n      .pred_monoceros,\n      .pred_musculus,\n      .pred_novaeangliae,\n      .pred_orca,\n      .pred_phocoena,\n      .pred_physalus,\n      .pred_sp.indet.,\n      .pred_truncatus,\n      levels = levels(species_levels),\n      min_prob = .9\n    )\n  )\n\nsubset_df |>\n  summarise(n = n(), .by = c(species, .class_pred)) |> \n  arrange(species, desc(n)) |> \n  rename(\"Actual\" = species, \"Predicted\" = .class_pred, \"Count\" = n)\n\n\n\n\nActual\nPredicted\nCount\n\n\n\nacutorostrata/borealis\n.pred_acutorostrata\n1\n\n\nacutorostrata?\n.pred_acutorostrata\n12\n\n\nacutus?\n.pred_acutus\n3\n\n\nalbirostris?\n.pred_albirostris\n9\n\n\nampullatus?\n.pred_ampullatus\n3\n\n\nbidens?\n.pred_bidens\n2\n\n\nbidens?\n[EQ]\n1\n\n\ncavirostris?\n.pred_cavirostris\n7\n\n\ncoeruleoalba?\n.pred_coeruleoalba\n1\n\n\ndelphis/coeruleoalba\n[EQ]\n48\n\n\ndelphis?\n.pred_delphis\n18\n\n\ngriseus?\n.pred_griseus\n2\n\n\nmacrocephalus?\n.pred_macrocephalus\n2\n\n\nmelaena?\n.pred_melaena\n20\n\n\norca?\n.pred_orca\n4\n\n\nphocoena?\n.pred_phocoena\n42\n\n\nphysalus/acutorostrata\n[EQ]\n1\n\n\nphysalus?\n.pred_physalus\n4\n\n\ntruncatus/albirostris\n[EQ]\n5\n\n\ntruncatus?\n.pred_truncatus\n18\n\n\n\n\n\n\nThe majority of the 203 uncertain records are predicted to be as suspected in the original labelling. The remainder are classed as equivocal as they have not met the high bar of a 0.9-or-above probability for a single species."
  },
  {
    "objectID": "project/cetacea/index.html#r-toolbox",
    "href": "project/cetacea/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[2]; as.integer[3]; c[13]; character[2]; factor[1]; integer[3]; is.na[1]; length[2]; levels[1]; library[14]; log10[1]; mean[2]; min[1]; names[1]; seq[3]; set.seed[4]; summary[1]\n\n\nclock\nas_iso_year_week_day[1]; date_build[1]; date_parse[1]; get_month[1]; get_week[1]; get_year[1]\n\n\ndoParallel\nregisterDoParallel[1]\n\n\ndplyr\nfilter[7]; across[2]; arrange[2]; case_when[1]; coalesce[1]; count[4]; desc[3]; if_else[6]; mutate[13]; n[4]; n_distinct[2]; recode[1]; rename[2]; row_number[1]; select[3]; slice_head[2]; summarise[3]\n\n\nfinetune\ncontrol_race[1]; plot_race[1]; tune_race_anova[1]\n\n\nforcats\nfct_drop[1]; fct_lump_n[1]; fct_reorder[1]\n\n\nggplot2\naes[6]; coord_flip[3]; coord_map[1]; element_text[2]; facet_wrap[2]; geom_jitter[2]; geom_map[1]; geom_violin[2]; ggplot[4]; labs[6]; map_data[1]; scale_colour_manual[2]; scale_fill_gradient2[1]; scale_fill_manual[1]; scale_size_manual[1]; scale_y_log10[1]; theme[3]; theme_bw[1]; theme_set[1]; theme_void[1]\n\n\nglue\nglue[3]\n\n\njanitor\nclean_names[1]\n\n\nparsnip\nboost_tree[1]; set_engine[1]; set_mode[1]\n\n\npatchwork\nwrap_plots[1]\n\n\nprobably\nclass_pred[1]; make_class_pred[1]\n\n\npurrr\nmap[2]\n\n\nreadr\nparse_number[1]; read_csv[1]\n\n\nrecipes\nall_nominal_predictors[1]; all_predictors[1]; has_role[2]; step_date[1]; step_dummy[1]; step_zv[1]; update_role[3]\n\n\nrsample\ninitial_split[1]; testing[1]; training[1]; vfold_cv[1]\n\n\nstats\nvar[3]\n\n\nstopwords\nstopwords[1]\n\n\nstringr\nstr_c[1]; str_detect[3]; str_remove_all[1]; str_to_title[2]\n\n\ntextrecipes\nstep_stopwords[1]; step_tf[1]; step_tokenfilter[1]; step_tokenize[1]\n\n\ntictoc\ntic[1]; toc[1]\n\n\ntidyr\ncrossing[1]\n\n\ntune\nfinalize_workflow[1]; select_best[1]\n\n\nvip\nvip[1]\n\n\nwesanderson\nwes_palette[1]\n\n\nworkflows\nadd_model[1]; add_recipe[1]; workflow[1]\n\n\nyardstick\naccuracy[2]; conf_mat[1]; metric_set[1]"
  },
  {
    "objectID": "project/deal/index.html",
    "href": "project/deal/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Before the post-Brexit trade negotiations concluded, what did quantitative textual analysis and word embeddings tell us about the shifting trade-talk sentiment?\nReading news articles on the will-they-won’t-they post-Brexit trade negotiations with the EU sees days of optimism jarred by days of gloom. Do negative news articles, when one wants a positive outcome, leave a deeper impression?\nIs it possible to get a more objective view from quantitative analysis of textual data? To do this, I’m going to look at hundreds of articles published in the Guardian newspaper over the course of the year to see how trade-talk sentiment changed week-to-week.\nThe Withdrawal Agreement between the UK and the European Union was signed on the 24th of January 2020. Brexit-related newspaper articles will be imported from that date.\nThe Guardian newspaper asks for requests to span no more than 1 month at a time. Creating a set of monthly date ranges will enable the requests to be chunked.\nThe data need a little cleaning, for example, to remove multi-topic articles, html tags and non-breaking spaces.\nA corpus then gives me a collection of texts whereby each document is a newspaper article.\nAlthough only articles mentioning Brexit have been imported, some of these will not be related to trade negotiations with the EU. For example, there are on-going negotiations with many countries around the world. So, word embeddings(Selivanov, Bickel, and Wang 2022) will help to narrow the focus to the specific context of the UK-EU trade deal.\nThe chief negotiator for the EU is Michel Barnier, so I’ll quantitatively identify words in close proximity to “Barnier” in the context of these Brexit news articles.\nWord embedding is a learned modelling technique placing words into a multi-dimensional vector space such that contextually-similar words may be found close by. Not surprisingly, one of the closest words contextually is “Michel”. And as he is the chief negotiator for the EU, we find “negotiator” and “brussels” also in the top most contextually-similar words.\nThe word embeddings algorithm, through word co-occurrence, has identified the name of Michel Barnier’s UK counterpart David Frost. So filtering articles for “Barnier”, “Frost” and “UK-EU” should help narrow the focus.\nQuanteda’s(Benoit et al. 2018) kwic function shows key phrases in context to ensure we’re homing in on the required texts. Short URLs are included below so one can click on any to read the actual article as presented by The Guardian.\nQuanteda provides a sentiment dictionary which, in addition to identifying positive and negative words, also finds negative-negatives and negative-positives such as, for example, “not effective”. For each week’s worth of articles, we can calculate the proportion of positive sentiments.\nPlotting the changing proportion of positive sentiment over time did surprise me a little. The outcome was more balanced than I expected which perhaps confirms the deeper impression left on me by negative articles.\nThe upper plot shows a rolling 7-day mean with a narrowing ribbon representing a narrowing variation in sentiment.\nThe lower plot shows the volume of articles. As we drew closer to the crunch-point the volume picked up."
  },
  {
    "objectID": "project/deal/index.html#r-toolbox",
    "href": "project/deal/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.Date[2]; c[5]; library[12]; mean[1]; set.seed[2]; sum[2]\n\n\nclock\nadd_days[1]; add_months[1]; date_build[1]; date_ceiling[1]\n\n\ndplyr\nfilter[2]; group_by[1]; left_join[2]; mutate[5]; n[2]; rename[1]; select[2]; slice[1]; slice_max[1]; slice_sample[1]; summarise[2]\n\n\nggplot2\naes[4]; geom_hline[1]; geom_line[2]; geom_ribbon[1]; ggplot[2]; labs[2]; scale_y_continuous[1]; theme_bw[1]; theme_set[1]\n\n\nglue\nglue[2]\n\n\nguardianapi\ngu_content[1]\n\n\npatchwork\nplot_layout[1]\n\n\npurrr\nlist_rbind[1]; pmap[1]; slowly[1]\n\n\nquanteda\nt[1]; corpus[2]; data_dictionary_LSD2015[1]; dfm[1]; fcm[1]; kwic[1]; phrase[1]; tokens[2]\n\n\nscales\nlabel_percent[1]\n\n\nslider\nslide_dbl[3]\n\n\nstats\nquantile[2]\n\n\nstringr\nstr_detect[2]; str_remove_all[3]; str_to_lower[1]\n\n\ntext2vec\nsim2[1]\n\n\ntibble\nas_tibble[3]; tibble[1]; rownames_to_column[1]\n\n\ntictoc\ntic[2]; toc[2]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/sw10/index.html",
    "href": "project/sw10/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Various events have impacted house sales in London. There has been a series of increases in stamp duty and the impact of the financial crisis. More recently Brexit and the consequences of Covid-19.\nHow is London postal area SW10 coping with all this?\nHouse prices paid data are provided by HM Land Registry Open Data.\nThe focus is on the standard price paid.\nA Telegraph article entitled Timeline: 20 years of stamp duty increases for home buyers pinpoints many of the key event dates.\nVisually, it appears that the financial crisis had a big impact on sales volume, with the Brexit vote sucking much of the remaining oxygen out of the market. Stamp duty increases in between probably slowed any intermediate recovery.\nAn alternative way of looking at this is by median quarterly prices (with upper and lower quartiles), supplemented by sales volumes.\nA ggmosaic (Jeppson, Hofmann, and Cook 2021) visualisation of the composition of SW10 reveals the postal area to be overwhelmingly dominated by leasehold flats.\nOther blog posts on quantum jitter look at SW10 property from diffferent perspectives: Digging Deep considers the correlation between house sales and planning applications; and Bootstraps & Bandings uses a sample of recent house sales to infer whether property bands are as representative of property values today as they were three decades ago."
  },
  {
    "objectID": "project/sw10/index.html#r-toolbox",
    "href": "project/sw10/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nSPARQL\nSPARQL[2]\n\n\nbase\nas.Date[1]; c[6]; factor[1]; library[10]; max[2]; min[1]; nrow[1]\n\n\nclock\nas_date[1]; as_year_quarter_day[1]; calendar_start[1]; date_format[1]; date_parse[1]; date_today[1]\n\n\ndplyr\nfilter[2]; mutate[3]; n[2]; pull[3]; recode[1]; rename[1]; summarise[4]\n\n\nggmosaic\ngeom_mosaic[1]; mosaic[1]; product[1]\n\n\nggplot2\naes[6]; annotate[4]; coord_cartesian[1]; element_blank[1]; element_text[1]; geom_hline[1]; geom_line[2]; geom_point[1]; geom_ribbon[1]; geom_smooth[1]; geom_vline[1]; ggplot[4]; labs[4]; scale_colour_manual[1]; scale_fill_manual[1]; scale_x_date[1]; scale_y_continuous[1]; scale_y_log10[1]; theme[2]; theme_bw[1]; theme_minimal[1]; theme_set[1]\n\n\nglue\nglue[4]\n\n\npatchwork\nplot_layout[1]\n\n\nscales\ncomma[1]; cut_short_scale[2]; label_dollar[2]\n\n\nstats\nmedian[1]; quantile[1]\n\n\nstringr\nstr_detect[1]; str_extract[2]; str_remove[1]\n\n\ntibble\nas_tibble[1]; tribble[1]\n\n\ntidyr\npivot_wider[1]\n\n\ntsibble\nscale_x_yearquarter[2]; tsibble[1]; yearquarter[1]\n\n\nvctrs\nnew_datetime[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/box/index.html",
    "href": "project/box/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Each project closes with a table summarising the R tools used. By visualising the most frequently used packages and functions I can get a sense of where I may most benefit from going deeper into the latest package versions.\nIt’s an opportunity to replace superseded functions e.g. spread and gather with pivot_wider and pivot_longer. Or to use newer packages, such as tidyclust which brings cluster modelling to tidymodels (used in the latest quarto version of Finding Happiness in ‘The Smoke’), or bslib used in the latest shiny app embedded in Plots Thicken.\nI’ll start by grabbing the url for all projects.\nThis enables me to extract the package and function usage table for each one.\nA little “spring cleaning” is needed, and separation of tidyverse and non-tidyverse packages.\nThen I can summarise usage and prepare for a faceted plot.\nClearly dplyr reigns supreme driven by mutate and filter.\nI’d also like a word cloud generated as the new featured image for this project."
  },
  {
    "objectID": "project/box/index.html#r-toolbox",
    "href": "project/box/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nA little bit circular, but I might as well include this code too in my “favourite things”.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[1]; as.integer[1]; c[8]; character[1]; integer[1]; library[11]; sample[1]; unique[1]\n\n\nclock\ndate_today[1]\n\n\ndplyr\nfilter[2]; arrange[1]; bind_rows[2]; case_when[1]; coalesce[1]; count[4]; desc[1]; mutate[6]; n_distinct[1]; pull[1]; select[1]; transmute[1]\n\n\nforcats\nfct_reorder[2]; fct_rev[1]\n\n\nfpp3\nfpp3_packages[1]\n\n\nggplot2\naes[7]; annotate[1]; coord_flip[2]; element_rect[1]; geom_col[3]; geom_label[3]; ggplot[4]; labs[2]; scale_colour_manual[1]; scale_fill_manual[3]; scale_size_area[1]; theme[2]; theme_bw[1]; theme_set[1]; theme_void[2]\n\n\nggwordcloud\ngeom_text_wordcloud[1]; ggwordcloud[1]\n\n\nglue\nglue[3]\n\n\njanitor\nclean_names[1]\n\n\npaletteer\npaletteer_c[1]\n\n\npatchwork\nplot_annotation[1]\n\n\npurrr\nlist_rbind[1]; map[1]\n\n\nrvest\nhtml_attr[1]; html_elements[2]; html_table[1]; read_html[2]\n\n\nstringr\nstr_c[1]; str_remove[2]; str_squish[1]\n\n\ntibble\nas_tibble[1]; tibble[1]\n\n\ntidymodels\ntidymodels_packages[1]\n\n\ntidyr\ndrop_na[1]; separate[1]; separate_rows[1]\n\n\ntidyverse\ntidyverse_packages[1]"
  },
  {
    "objectID": "project/thicken/index.html",
    "href": "project/thicken/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "One could think of data science as “art grounded in facts”. It tells a story through visualisation. Both story and visualisation rely on a good plot. And an abundance of those has evolved over time. Many have their own dedicated Wikipedia page 😲.\nWhich generate the most interest? How is the interest trending over time? Let’s build an interactive app to find out.\nI’m going to start by harvesting some data from Wikipedia’s Statistical charts and diagrams category. I can use this to build a list of all chart types which have a dedicated Wikipedia article page. Using rvest (Wickham 2022) inside the app ensures it will respond to any newly-created articles.\nThe pageviews (Keyes and Lewis 2020) package provides an API into Wikipedia. I’ll create a function wrapped around article_pageviews so I can later iterate through a subset of the list established in the prior code chunk.\nI want an input selector so that a user can choose plot types for comparison. I also want to provide user control of the y-axis scale. A combination of fixed and log10 is better for comparing plots. Free scaling reveals more detail in the individual trends.\nAlthough shinyuieditor (Strayer 2022) is currently in Alpha at the time of this update, using launch_editor(\"/content/project/thicken/\") helped me modify the basic grid layout of the UI for this pre-existing app.R.\nThe server component outputs a faceted ggplot.\nAnd here’s the live shiny (Chang et al. 2022) app deployed via shinyapps.io.\nNote the utility of selecting the right scaling. The combination of “fixed” and “normal” reveals what must have been “world histogram day” on July 27th 2015, but little else.\nTurning non-interactive code into an app sharpens the mind’s focus on performance. And profvis (Chang, Luraschi, and Mastny 2020), integrated into RStudio via the profile menu option, is a wonderful “tool for helping you understand how R spends its time”.\nMy first version of the app was finger-tappingly slow.\nProfvis revealed the main culprit to be the pre-loading of a dataframe with the page-view data for all chart types (there are more than 100). Profiling prompted the more efficient “reactive” approach of loading the data only for the user’s selection (maximum of 8).\nProfiling also showed that rounding the corners of the plot.background with additional grid-package code was expensive. App efficiency felt more important than minor cosmetic detailing (to the main panel to match the theme’s side panel). And most users would probably barely notice (had I not drawn attention to it here)."
  },
  {
    "objectID": "project/thicken/index.html#r-toolbox",
    "href": "project/thicken/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[6]; library[9]; list[1]; switch[1]\n\n\nbslib\nbs_theme[1]\n\n\ndplyr\nfilter[1]; mutate[1]\n\n\nggplot2\naes[1]; element_text[1]; facet_wrap[1]; geom_line[1]; geom_smooth[1]; ggplot[1]; labs[1]; margin[1]; scale_colour_manual[1]; scale_y_log10[1]; theme[1]; theme_bw[1]; theme_set[1]\n\n\ngridlayout\ngrid_card[1]; grid_card_plot[1]; grid_card_text[1]; grid_page[1]\n\n\nlubridate\ntoday[1]; ymd[1]\n\n\npageviews\narticle_pageviews[1]\n\n\npurrr\nlist_rbind[1]; map[1]\n\n\nrvest\nhtml_elements[1]; html_text[1]; read_html[1]; session[1]\n\n\nscales\ncut_short_scale[1]; label_number[1]\n\n\nshiny\ndateRangeInput[1]; reactive[1]; renderPlot[1]; req[1]; selectInput[2]; selectizeInput[1]; shinyApp[1]\n\n\nstringr\nstr_c[1]; str_replace_all[1]\n\n\ntibble\ntibble[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/planning/index.html",
    "href": "project/planning/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "In House Sales I looked at how a series of events damped down sales. By combining these sales data with planning applications I’d like to see if home owners “start digging” when they can’t sell.\nPlanning data is harvested with the kind permission of The Royal Borough of Kensington and Chelsea (RBKC). The code for these code chunks is not rendered out of courtesy to RBKC.\nThe data need a bit of wrangling. And there is also the opportunity to try the newest column-wise enhancements to mutate: mutate_if and mutate_at have been superseded by mutate with across.\nquanteda (Benoit et al. 2018) to look at key words in context (kwic).\nI’d like to review planning applications by theme. So I’ll first need to get a sense of what the themes are by plotting the words which appear most frequently.\nNow I can create a theme feature.\nI also want to compare house sales with planning applications over time. So, I’ll re-use the SPARQL query from House Sales.\nLet’s now bind the data into one tibble and summarise the transaction volumes over time.\nThe visualisation below does suggest that home owners “start digging” when they can’t sell. At least in this part of London.\nTime-series data may have an underlying trend and a seasonality pattern. I’ll use the seasonal package to decompose each time-series. Each exhibit annual seasonality which evolves over time.\nWe also see some inverse correlation between the two time-series re-affirming the visual conclusion that planning applications increase when the housing market is depressed.\nThe overall volumes of planning applications and house transactions in SW10 are fairly similar.\nEarlier, I added a “theme” feature to the data. So let’s take a look at the volume of applications over time faceted by theme and coloured by the outcome. We see that the rise in planning applications is fuelled by basements or excavations, and work on outside landscaping and terracing. So perhaps we do “dig” when we can’t sell."
  },
  {
    "objectID": "project/planning/index.html#r-toolbox",
    "href": "project/planning/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nDT\ndatatable[1]\n\n\nSPARQL\nSPARQL[2]\n\n\nbase\nas.numeric[1]; basename[1]; c[17]; character[1]; factor[1]; file[1]; is.character[1]; is.na[3]; library[12]; min[1]; numeric[1]; readRDS[1]; saveRDS[1]; sum[2]; url[1]\n\n\nclock\ndate_build[1]; date_parse[1]; get_month[1]; get_year[2]\n\n\ndplyr\nfilter[2]; across[4]; arrange[1]; bind_rows[2]; case_when[3]; count[1]; desc[1]; if_else[3]; left_join[1]; mutate[9]; n[4]; na_if[2]; rename[2]; select[2]; slice_head[1]; summarise[4]\n\n\nfabletools\ncomponents[1]; model[1]\n\n\nfeasts\nCCF[1]; STL[1]\n\n\nforcats\nfct_explicit_na[1]; fct_lump[1]; fct_reorder[1]\n\n\nggplot2\naes[3]; coord_flip[1]; element_text[1]; facet_wrap[1]; geom_bar[1]; geom_col[1]; geom_line[1]; ggplot[3]; labs[5]; scale_colour_manual[2]; scale_fill_manual[1]; theme[1]; theme_bw[1]; theme_set[1]\n\n\npurrr\nlist_rbind[2]; map[2]\n\n\nquanteda\ncorpus[2]; dfm[1]; kwic[1]; phrase[1]; stopwords[1]\n\n\nquanteda.textstats\ntextstat_frequency[1]\n\n\nreadr\nread_csv[1]\n\n\nrvest\nhtml_attr[1]; html_element[2]; html_elements[2]; html_table[1]; html_text[1]; read_html[3]\n\n\nstats\nfrequency[2]\n\n\nstringr\nstr_c[3]; str_detect[11]; str_extract[1]; str_remove_all[2]; str_replace[2]; str_squish[1]; str_to_lower[1]; str_to_upper[1]; str_trim[1]\n\n\ntibble\nas_tibble[2]; tibble[1]\n\n\ntictoc\ntic[2]; toc[2]\n\n\ntidyr\npivot_wider[2]; replace_na[1]\n\n\ntsibble\nyearmonth[1]\n\n\nutils\ndownload.file[1]\n\n\nvctrs\nnew_datetime[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/bands/index.html",
    "href": "project/bands/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Are the residential property bands of 3 decades ago becoming less so? Would a sample of those recently-sold reveal band convergence? And what may be inferred about those not sampled?\nOver the years, urban properties have been added to and divided up. And two streets of equal attractiveness, and with equivalently-banded properties, may have diverged as neighbourhoods evolved.\nWhilst properties can and do move to higher or lower bands following alteration, would a sample of those recently-sold reveal band convergence after so long? And what may be inferred about the wider housing stock?\nSetting the theme and colour palette for all graphics (with a little help from the ggfx package).\nProperty band and price-paid data are separately sourced. The free-form street address is the only way to bring the two together. The structure, content and even spelling of the address sometimes differ, for example: “FLAT C, 22 SOME STREET, SOME AREA, SW10 1AA” in one may be “22C 2ND FLR, HOUSE NAME, SOME STREET SW10 1AA” in the other.\nSo, a little string manipulation is needed to create a common key. And reusable patterns will enable a consistent application to both.\nRegex is used here to isolate pieces of text:\nCouncil Tax band data are available for non-commercial use1.\nHouse price-paid data are similarly available for non-commercial use2.\nNow there’s a common key to join the data.\nAs with previous posts Digging Deep and House Sales, I’m focusing on postcodes in the SW10 part of London.\nIt’s not possible to assess all SW10 properties by band since only a tiny fraction will have been sold recently. Recent sales could though be used as a sample and Bootstrap Confidence Intervals then employed to draw a wider inference.\n“Pulling yourself up by your bootstraps” originally meant doing something absurd. Later it came to mean succeeding with only what you have at your disposal. Hence only the sample will be used as a surrogate for the true population by making repeated random draws from it (with replacement).\nA key assumption is that the sample is representative of the true population.\nEven though only recent sales transactions have been selected, a small movement in market prices will have occurred. So ensuring the bands are reasonably well distributed over the period is worthwhile.\nA violin plot of the property values by band shows some bimodal distribution and oddly shows bands E & F with lower mean prices than band D. This is worth closer inspection to ensure the sample is representative.\nIt turns out that the unusual transactions below £0.3m are almost entirely from one postcode as shown below when isolating “SW10 0JR”. This appears to be a single large new development with all sub-units sold in 2020.\nThese specific transactions feel somewhat unusual at these banding levels. And irrespective of their accuracy, a sample of 169 postcodes heavily dominated by the transactions of just one would not be representative of the true population.\nSo, I’ll remove this postcode.\nThis now feels like a representative sample of 312 property transactions. And broadly-speaking the plot shows a progression in average property values as we step through the bands. There is though substantial convergence between some, with the “drippy” band E still looking almost indistinguishable from band D.\nCan we infer that the true population of band Es no longer exhibits any difference in mean values with respect to band D?\nBootstrapping with a 95% confidence interval suggests the true difference in mean prices between all band D and E properties in SW10 is somewhere in the range -£0.10m to £0.10m. Considerable convergence compared to 3 decades ago when the band E minimum exceeded the band D maximum."
  },
  {
    "objectID": "project/bands/index.html#r-toolbox",
    "href": "project/bands/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nRColorBrewer\nbrewer.pal[1]\n\n\nSPARQL\nSPARQL[2]\n\n\nbase\nc[6]; factor[1]; library[12]; max[1]; mean[2]; min[1]; readRDS[1]; saveRDS[1]; scale[3]; seq[1]; set.seed[2]\n\n\nclock\nas_date[1]\n\n\ndplyr\nfilter[4]; across[1]; arrange[1]; count[4]; if_else[1]; inner_join[1]; mutate[7]; n[2]; pull[7]; relocate[1]; rename_with[1]; select[4]; slice_head[1]; slice_sample[1]; summarise[4]\n\n\nggfx\nas_reference[1]; with_blend[1]; with_outer_glow[1]\n\n\nggplot2\naes[8]; annotate[1]; coord_flip[1]; geom_col[2]; geom_hline[1]; geom_label[3]; geom_text[1]; geom_violin[3]; geom_vline[1]; ggplot[5]; labs[5]; position_fill[1]; scale_fill_distiller[1]; scale_fill_manual[2]; scale_x_continuous[1]; scale_y_continuous[1]; scale_y_log10[3]; theme_bw[1]; theme_set[1]; theme_void[1]\n\n\nglue\nglue[9]\n\n\ninfer\ncalculate[2]; generate[1]; get_ci[1]; shade_confidence_interval[1]; specify[2]; visualise[1]\n\n\njanitor\nclean_names[2]\n\n\npurrr\nlist_rbind[1]; map2[1]; possibly[1]\n\n\nrvest\nhtml_element[1]; html_table[1]; read_html[1]\n\n\nscales\ndollar[6]; label_dollar[4]; label_percent[1]\n\n\nstringr\nstr_c[4]; str_extract[1]; str_remove[1]; str_remove_all[4]; str_replace[2]; str_replace_na[1]; str_squish[2]\n\n\ntibble\nas_tibble[1]; tibble[1]\n\n\ntidyr\ncrossing[1]; fill[1]\n\n\ntsibble\nscale_x_yearquarter[1]; tsibble[1]; yearquarter[3]\n\n\nvctrs\nnew_datetime[1]"
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "Little Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Updated - Oldest\n        \n         \n          Updated - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nA Footnote in History\n\n\n7 min\n\n\nThe grammar of tables, footnotes and occupations consigned to history\n\n\n\nNov 1, 2022\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Happiness in The Smoke\n\n\n8 min\n\n\nCluster analysis and the characteristics that bind London boroughs\n\n\n\nMar 19, 2022\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nBootstraps & Bandings\n\n\n13 min\n\n\nDecades-old residential property bands and inference using a sample of those recently sold\n\n\n\nMar 8, 2022\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSea Monsters that Lost their Way\n\n\n12 min\n\n\nPredicting uncertain species of cetacean strandings recorded by the Natural History Museum\n\n\n\nDec 4, 2021\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Frosty Deal?\n\n\n6 min\n\n\nQuantitative textual analysis, word embeddings and analysing shifting trade-talk sentiment?\n\n\n\nSep 18, 2020\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Goldilocks Principle\n\n\n4 min\n\n\nSimulating stock portfolio returns inspired by bowls of porridge left by three bears\n\n\n\nAug 9, 2020\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeathering the Storm\n\n\n2 min\n\n\nTimeseries comparison and the impact of Covid-19 on the financial markets by sector\n\n\n\nAug 2, 2020\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nFavourite Things\n\n\n4 min\n\n\nR packages & functions that make doing data science a joy based on usage across projects\n\n\n\nJul 26, 2020\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nEast-West Drift\n\n\n5 min\n\n\nAnimated dimension reduction and East-West historical UN voting patterns\n\n\n\nJan 9, 2019\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeeing the Wood for the Trees\n\n\n4 min\n\n\nVisualising small multiples when crime data leave you unable to see the wood for the trees\n\n\n\nJan 1, 2019\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan Ravens Forecast?\n\n\n7 min\n\n\nTime series forecasting using cloud services spend data\n\n\n\nJul 29, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSix Months Later\n\n\n5 min\n\n\nExploring colour palettes and small multiples using cloud services spend data\n\n\n\nApr 2, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCriminal Goings-on in a Random Forest\n\n\n10 min\n\n\nCriminal goings-on in a random forest and predictions with tree-based and glm models\n\n\n\nMar 1, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlots Thicken\n\n\n6 min\n\n\nEvery story needs a good plot. Which plot types generate the most interest on Wikipedia?\n\n\n\nFeb 7, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nCluster of Six\n\n\n8 min\n\n\nExploring parliamentary voting patterns with hierarchical clustering\n\n\n\nJan 29, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigging Deep\n\n\n9 min\n\n\nDo we see more planning applications when house sales are depressed?\n\n\n\nJan 10, 2018\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurprising Stories\n\n\n4 min\n\n\nA little interactive geospatial mapping and an unexpected find\n\n\n\nDec 20, 2017\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nHouse Sales\n\n\n6 min\n\n\nA series of events, such as the Financial Crisis and the 2016 Brexit vote, that damped down residential property sales in London\n\n\n\nDec 17, 2017\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere Clouds Cross\n\n\n8 min\n\n\nVisualising the dozens of overlapping sets formed by categories of cloud services\n\n\n\nNov 14, 2017\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s Jitter\n\n\n4 min\n\n\nWelcome to the tidyverse with data ingestion, cleaning and tidying. And some visualisations of sales data with a little jittering.\n\n\n\nSep 12, 2017\n\n\n\n\n\nDec 23, 2022\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "project/goldilocks/index.html",
    "href": "project/goldilocks/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "The Goldilocks principle has its origins in a children’s story about a girl who tastes the bowls of porridge left by three bears. She prefers the one that is neither too hot nor too cold, but is just right.\nWhen it comes to investing in stocks, how many is “just right”?\nI’ll use this palette.\nSuppose the average stock market return is around 10%. And you do extensive research, burning the midnight oil, poring over stock fundamentals. Or perhaps you develop a cool machine learning model. And you arrive at a list of 50 promising stocks you feel confident would, on average, deliver well-above-market returns.\nI’ll create some randomly made up stocks with an average return close to 40%. Some will tank due to events one could not foresee; I’ll allow some to lose up to 20%. Similarly, some could generate exceptional returns.\nHere’s the resultant distribution I’ll use to assess the impact of portfolio size. Stock markets are fairly close to a normal distribution, albeit with fatter tails due to a few extreme outcomes.\nNow suppose you share 2 stocks, selected at random, with 1,000 of your social network friends (selecting a different pair of stocks for each friend). Will they all still be friends a year later? And if you repeated the same scenario with portfolio sizes of 5, 10, 20 and 50 stocks per person, would that change the outcome? Let’s see.\nSo, for all portfolio sizes, the average return across your 1,000 friends is around 42%.\nBut, when the portfolio size is 2, you may be erased from a few Christmas card lists (or worse). If one of those two stocks has an extreme negative outcome, there’s little else in the portfolio to dissipate the effect. As the portfolio size increases, the risk (downside and upside) dramatically reduces.\nBut is more always better? Well, irrespective of whether your list of promising stocks resulted from desk research or a model, there will be a varying degree of confidence in the 50. A machine learning model, for example, would assign class probabilities to each stock.\nSo by picking a smaller number, one can select those in which one feels most confident, or which have the highest class probability. And by picking a larger number (ideally across different sectors to further reduce risk) one can weaken the effects of a bad egg or two caused by events no research or model could foresee.\nSo perhaps the answer is to pick a worst-case scenario one would be prepared to accept. In the plot above, accepting a small chance of only a 12% return (still better than the historical average market return), might provide the “just right” portfolio. A portfolio of a manageable size, focused on your highest-confidence stocks, and with pretty good odds of the desired return."
  },
  {
    "objectID": "project/goldilocks/index.html#r-toolbox",
    "href": "project/goldilocks/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[2]; chartr[1]; factor[1]; library[4]; mean[3]; min[1]; rep[5]; return[2]; sample[1]; set.seed[2]\n\n\ndplyr\nbind_rows[1]; mutate[1]; slice_sample[1]; summarise[2]\n\n\nggplot2\naes[5]; geom_histogram[1]; geom_label[2]; geom_violin[1]; ggplot[2]; labs[2]; scale_fill_manual[1]; scale_x_continuous[1]; scale_y_continuous[1]; theme_bw[1]; theme_set[1]\n\n\npurrr\nlist_rbind[1]; map[1]\n\n\nscales\nbreaks_extended[1]; label_percent[2]; percent[2]\n\n\ntibble\ntibble[1]\n\n\ntruncnorm\nrtruncnorm[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/stories/index.html",
    "href": "project/stories/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Late in 2017 I experimented with geospatial mapping techniques in R. The log file for my blog seemed like a good source of data. I thought it might appeal to a wider audience of one (including me).\nCombined with longitude and latitude data from MaxMind’s GeoLite2, this offered a basis for analysis. Although less precise than the GeoIP2 database, this would be more than adequate for my purpose of getting to country and city level. I settled on the leaflet (Cheng, Karambelkar, and Xie 2022) package for visualisation given the interactivity and pleasing choice of aesthetics.\nThe results however were a little puzzling.\nThe concentration of page views in central London was of no immediate surprise as this was likely to be my site maintenance and blogging. What did strike me as odd though was the high concentration of page views in the centre of the US. More curious still, when I zoomed in on Kansas and found myself in the middle of the Cheney Reservoir.\nI imagined someone drifting in the expanse of water with laptop, flask of coffee and box of sandwiches, whiling away the hours absorbed in my blog. Perhaps not. How could such a small number of blog pages generate in excess of 2,000 page views in one spot in less than two months?\nThen I chanced upon a BBC news article from August 2016. When unable to locate IPs, MaxMind chose the geographical centre of the US as a default. This initially turned out to be a rented house in Kansas, which was rather unfortunate for the occupants, and brought upon them all kinds of unwanted attention.\nMaxMind subsequently changed its default centre points to be the middle of bodies of water. And this solved another puzzle. Some of the page views in London appeared to be in the middle of the River Thames."
  },
  {
    "objectID": "project/stories/index.html#r-toolbox",
    "href": "project/stories/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nR.utils\nR.utils[1]; gunzip[1]\n\n\nbase\nas.character[1]; basename[1]; c[5]; character[1]; file[2]; getwd[1]; is.na[2]; library[7]; list[1]; url[1]\n\n\ndplyr\nfilter[1]; arrange[1]; case_when[2]; if_else[2]; mutate[3]; rename[1]\n\n\nggplot2\ntheme_bw[1]; theme_set[1]\n\n\nhtmlwidgets\nsaveWidget[1]\n\n\nleaflet\naddCircleMarkers[1]; addLegend[1]; addPolygons[1]; addProviderTiles[1]; colorFactor[1]; highlightOptions[1]; labelOptions[1]; leaflet[2]; providerTileOptions[1]; setView[1]\n\n\npurrr\nlist_rbind[1]; map2[1]\n\n\nreadr\nread_csv[1]\n\n\nrgdal\nreadOGR[1]\n\n\nrgeolocate\nmaxmind[1]\n\n\nstringr\nstr_c[2]\n\n\nutils\ndownload.file[2]; unzip[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/happiness/index.html",
    "href": "project/happiness/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "The Smoke, to use London’s nickname, has 32 boroughs plus the central business district known as the City of London. What does Cluster Analysis tell us about the characteristics that bind them?\nThe graphics will use a custom palette created in Adobe Colour.\nThe London Datastore provides data profiling each area."
  },
  {
    "objectID": "project/happiness/index.html#dimensionality-reduction",
    "href": "project/happiness/index.html#dimensionality-reduction",
    "title": "Toolbox",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nThese data include 81 numeric variables quantifying such things as population density, happiness and age. Way too many variables to visualise two-dimensionally. Principal Components Analysis can reduce the bulk of the information down to two variables. It is then possible to more easily visualise the relationships.\nThe City of London, aka “The Square Mile”, is quite distinct from the other 32 areas and has many NA values.\n\nraw_df |> \n  rowwise() |> \n  mutate(na_count = sum(is.na(cur_data()))) |> \n  select(area_name, na_count) |>\n  filter(na_count != 0) |>\n  arrange(desc(na_count))\n\n\n\n\narea_name\nna_count\n\n\n\nCity of London\n27\n\n\nKensington and Chelsea\n3\n\n\nBarnet\n1\n\n\nCamden\n1\n\n\nHackney\n1\n\n\nHaringey\n1\n\n\nHarrow\n1\n\n\nIslington\n1\n\n\nLewisham\n1\n\n\nMerton\n1\n\n\nRichmond upon Thames\n1\n\n\nWaltham Forest\n1\n\n\nWandsworth\n1\n\n\n\n\n\n\nNot surprisingly, the two-dimensional visualisation sets the City of London apart. And the other 32 are broadly, albeit with some mixing, divided into inner and outer London boroughs.\n\npca_fit <- raw_df |>\n  select(where(is.numeric)) |>\n  prcomp(scale = TRUE)\n\npca_augmented <-\n  pca_fit |>\n  augment(raw_df)\n\npca_augmented |>\n  ggplot(aes(.fittedPC1, .fittedPC2, fill = inner_outer_london)) +\n  geom_label(aes(label = area_name), size = 2, hjust = \"inward\") +\n  scale_fill_manual(values = as.character(cols)) +\n  labs(\n    title = \"33 London Areas\", fill = \"London\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nAfter squeezing the many dimensions into two, how much of the original information was it possible to retain?\n\npca_tidied <- pca_fit |>\n  tidy(matrix = \"eigenvalues\")\n\npct_explained <-\n  pca_tidied |>\n  pluck(\"cumulative\", 2)\n\npca_tidied |>\n  ggplot(aes(PC, percent)) +\n  geom_col(aes(fill = if_else(PC <= 2, TRUE, FALSE)),\n    alpha = 0.8, show.legend = FALSE\n  ) +\n  scale_y_continuous(labels = label_percent(1)) +\n  scale_fill_manual(values = as.character(cols)) +\n  coord_flip() +\n  labs(\n    title = glue(\n      \"{percent(pct_explained, 0.1)} of the \",\n      \"Variance Explained by Principal Components 1 & 2\"\n    ),\n    x = \"Principal Component\", y = NULL\n  )\n\n\n\n\nWhilst we do lose ease of interpretation by distilling the information in this way, it is still possible to understand which of the original variables influenced their two-dimensional positioning.\nThe axes depicted by the arrows below tell us that anxiety scores play a significant role in the placement of the City of London towards the upper-left. Average age pushes areas more towards the top. And happiness influences the bottom-right.\n\npattern <- \"_\\\\d{4}|_st.+|_score|_rates|proportion_of_|_\\\\d{2}_out_of_\\\\d{2}\"\n\npca_fit |>\n  tidy(matrix = \"rotation\") |>\n  pivot_wider(names_from = \"PC\", names_prefix = \"PC\", values_from = \"value\") |>\n  mutate(column = str_remove_all(column, pattern)) |>\n  ggplot(aes(PC1, PC2)) +\n  geom_segment(\n    xend = 0, yend = 0, colour = \"grey70\",\n    arrow = arrow(ends = \"first\", length = unit(8, \"pt\"))\n  ) +\n  geom_text_repel(aes(label = column), size = 3) +\n  theme_minimal() +\n  labs(\n    x = \"PC 1\", y = \"PC 2\",\n    title = \"Characteristics Influencing Area Positioning\",\n    caption = \"Source: data.london.gov.uk\"\n  ) +\n  theme(axis.text = element_blank())\n\n\n\n\nThis may be validated by ranking all 33 areas by these three original variables.\n\npca_long <- \n  pca_augmented |>\n  select(area_name, matches(\"happ|anx|average_age\")) |>\n  rename_with(~ str_remove(., \"_.*\")) |>\n  rename(\"avg_age\" = \"average\") |>\n  pivot_longer(-area, values_to = \"score\") |>\n  mutate(area = reorder_within(area, score, name)) \n\npca_long |>\n  ggplot(aes(area, score, colour = name)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~name, scales = \"free\") +\n  scale_x_reordered() +\n  scale_colour_manual(values = as.character(cols)) +\n  coord_flip() +\n  labs(x = NULL, caption = \"Source: data.london.gov.uk\")"
  },
  {
    "objectID": "project/happiness/index.html#cluster-modelling",
    "href": "project/happiness/index.html#cluster-modelling",
    "title": "Toolbox",
    "section": "Cluster Modelling",
    "text": "Cluster Modelling\nTo collect these areas into their natural groupings, a decision is needed on the desired number of clusters. We can visualise dividing the areas into 1, 2, 3 and so forth clusters. And, per below, 3 appears to nicely capture the natural grouping of the coloured points.\n\nset.seed(2022)\n\nkclusts <-\n  tibble(k = 1:6) |>\n  mutate(\n    kclust = map(k, ~ kmeans(pca_augmented |> \n                               select(.fittedPC1, .fittedPC2), .x)),\n    tidied = map(kclust, tidy),\n    glanced = map(kclust, glance),\n    augmented = map(kclust, augment, pca_augmented)\n  )\n\nassignments <-\n  kclusts |>\n  unnest(cols = c(augmented))\n\nclusters <-\n  kclusts |>\n  unnest(cols = c(tidied))\n\nassignments |>\n  ggplot(aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(color = .cluster)) +\n  facet_wrap(~k, nrow = 2) +\n  scale_colour_manual(values = as.character(cols[c(1:6)])) +\n  geom_point(data = clusters, size = 4, shape = 13) +\n  labs(\n    title = \"How Many Clusters Best Captures the Groupings?\",\n    subtitle = \"X Marks the Cluster Centre\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nThe elbow method provides a more mathematical approach to the choice. The compactness of the clustering (as measured by the total within-cluster sum of squares) is significantly optimised when choosing 3 clusters, with diminishing returns thereafter.\n\nkclusts |>\n  unnest(cols = c(glanced)) |>\n  ggplot(aes(k, tot.withinss)) +\n  geom_line() +\n  geom_point() +\n  geom_label(aes(label = if_else(k == 3, \"Elbow\", NA_character_)),\n    nudge_y = -25, fill = cols[1]\n  ) +\n  labs(\n    title = \"Elbow Method\",\n    x = \"Clusters\", y = \"Within-Cluster Variance\"\n  )\n\n\n\n\nAnd settling on this choice of 3 clusters, we get this split.\n\nassignments |>\n  filter(k == 3) |>\n  ggplot(aes(.fittedPC1, .fittedPC2, fill = .cluster)) +\n  geom_label(aes(label = area_name), \n             size = 2, hjust = \"inward\", overlap = FALSE) +\n  scale_fill_manual(values = as.character(cols[c(1, 2, 4)])) +\n  labs(\n    title = \"Closely-Related London Areas\", fill = \"Cluster\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    caption = \"Source: data.london.gov.uk\"\n  )"
  },
  {
    "objectID": "project/happiness/index.html#using-tidymodels",
    "href": "project/happiness/index.html#using-tidymodels",
    "title": "Toolbox",
    "section": "Using Tidymodels",
    "text": "Using Tidymodels\nAn alternative approach is to use the new tidyclust(Hvitfeldt and Bodwin 2022) package which augments the tidymodels framework with a tidy unified interface to clustering models.\nFirst we tune the model with 1 to 6 clusters and review how well they capture the natural groupings.\n\nkmeans_spec <- k_means(num_clusters = tune()) |> \n  set_engine(\"stats\", algorithm = \"Hartigan-Wong\")\n\nkmeans_rec <- recipe(~ .fittedPC1 + .fittedPC2, data = pca_augmented)\n\nkmeans_wflow <- workflow() |>\n  add_model(kmeans_spec) |>\n  add_recipe(kmeans_rec)\n\nkmeans_cv <- vfold_cv(pca_augmented, v = 5, repeats = 10)\n\nkmeans_res <- tune_cluster(\n  kmeans_wflow,\n  resamples = kmeans_cv,\n  grid = crossing(\n    num_clusters = seq(1, 6, 1)\n  ),\n  control = control_grid(save_pred = TRUE, extract = identity),\n  metrics = cluster_metric_set(sse_total, sse_ratio)\n)\n\nkmeans_metrics <- kmeans_res |> collect_metrics()\n\nkmeans_metrics |>\n  filter(.metric == \"sse_ratio\") |>\n  ggplot(aes(num_clusters, mean)) +\n  geom_point() +\n  geom_line() +\n  geom_label(aes(label = if_else(num_clusters == 3, \"Elbow\", NA_character_)),\n             nudge_y = -0.1, fill = cols[1]) +\n  labs(title = \"Elbow Method\", x = \"Clusters\", y = \"WSS\") +\n  scale_x_continuous(breaks = 1:6)\n\n\n\n\nAgain we can visualise the 3 clusters suggested by the elbow method.\n\nkmeans_spec <- k_means(num_clusters = 3) |> \n  set_engine(\"stats\", algorithm = \"Hartigan-Wong\")\n\nkmeans_wflow <- kmeans_wflow |> \n  update_model(kmeans_spec)\n\nkmeans_fit <- kmeans_wflow |> \n  fit(pca_augmented) \n\nkmeans_clust <- kmeans_fit |> \n  extract_centroids()\n\nkmeans_aug <- kmeans_fit |> \n  augment(pca_augmented)\n\nkmeans_aug |>\n  ggplot(aes(.fittedPC1, .fittedPC2)) +\n  geom_label(aes(label = area_name, colour = .pred_cluster),\n             size = 2, hjust = \"inward\") +\n  scale_colour_manual(values = as.character(cols[c(1:3, 5)])) +\n  geom_point(data = kmeans_clust, size = 4, shape = 13) +\n  labs(\n    title = \"Closely-Related London Areas\", fill = \"Cluster\",\n    subtitle = \"X Marks the Cluster Centre\",\n    x = \"Principal Component 1\", y = \"Principal Component 2\",\n    caption = \"Source: data.london.gov.uk\"\n  )\n\n\n\n\nHow does this look with geospatial data? And how do the clusters relate to inner and outer London?\n\nshape_df <-\n  st_read(\"statistical-gis-boundaries-london/ESRI\",\n    \"London_Borough_Excluding_MHW\",\n    as_tibble = TRUE, quiet = TRUE\n  ) |>\n  left_join(assignments |> \n              filter(k == 3), by = c(\"GSS_CODE\" = \"code\")) |>\n  select(.cluster, inner_outer_london, NAME, geometry) |>\n  pivot_longer(c(.cluster, inner_outer_london)) |>\n  mutate(value = recode(value, \"1\" = \"Cluster 1\", \n                        \"2\" = \"Cluster 2\", \"3\" = \"Cluster 3\"))\n\nshape_df |>\n  mutate(name = recode(name,\n    \".cluster\" = \"By Cluster\",\n    \"inner_outer_london\" = \"By Inner/Outer\"\n  )) |>\n  ggplot() +\n  geom_sf(aes(fill = value), colour = \"white\") +\n  geom_sf_label(aes(label = NAME), size = 2, alpha = 0.7) +\n  scale_fill_manual(values = as.character(cols[c(3, 4, 1, 2, 5)])) +\n  facet_wrap(~name) +\n  theme_void() +\n  theme(legend.position = \"none\") +\n  labs(fill = NULL)\n\n\n\n\nNot too dissimilar, but with some notable differences.\nThe City of London is a cluster apart in the heart of London. Kensington and Chelsea is an inner-London borough, but exhibits outer-London characteristics. And the reverse is true of the likes of Brent and Greenwich.\nDimensionality reduction is further explored in East-West Drift coupled with animation."
  },
  {
    "objectID": "project/happiness/index.html#r-toolbox",
    "href": "project/happiness/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[8]; c[10]; character[9]; identity[1]; is.character[1]; is.na[1]; is.numeric[1]; library[10]; mean[1]; numeric[1]; seq[1]; set.seed[1]; sum[1]\n\n\ndplyr\nfilter[5]; across[1]; arrange[1]; cur_data[1]; desc[1]; if_else[3]; left_join[1]; mutate[7]; na_if[1]; recode[2]; rename[1]; rename_with[1]; rowwise[1]; select[5]\n\n\nforcats\nfct_inorder[1]\n\n\nggplot2\naes[21]; annotate[1]; arrow[1]; coord_flip[2]; element_blank[1]; facet_wrap[3]; geom_col[2]; geom_label[6]; geom_line[2]; geom_point[6]; geom_segment[1]; geom_sf[1]; geom_sf_label[1]; ggplot[11]; labs[10]; scale_colour_manual[3]; scale_fill_manual[5]; scale_x_continuous[1]; scale_y_continuous[1]; theme[3]; theme_bw[1]; theme_minimal[1]; theme_set[1]; theme_void[2]; unit[1]\n\n\nggrepel\ngeom_text_repel[1]\n\n\nglue\nglue[2]\n\n\njanitor\nclean_names[1]\n\n\npurrr\nmap[4]; pluck[1]\n\n\nreadxl\nread_xlsx[1]\n\n\nrecipes\nrecipe[1]\n\n\nrsample\nvfold_cv[1]\n\n\nscales\nlabel_percent[1]; percent[2]\n\n\nsf\nst_read[1]\n\n\nstats\nkmeans[1]; prcomp[1]\n\n\nstringr\nstr_remove[2]; str_remove_all[1]; str_starts[1]\n\n\ntibble\ntibble[2]\n\n\ntidyclust\ncluster_metric_set[1]; extract_centroids[1]; k_means[2]; sse_ratio[1]; tune_cluster[1]\n\n\ntidyr\ncrossing[1]; pivot_longer[2]; pivot_wider[1]; unnest[3]\n\n\ntidytext\nreorder_within[1]; scale_x_reordered[1]\n\n\ntune\ncontrol_grid[1]\n\n\nworkflows\nadd_model[1]; add_recipe[1]; update_model[1]; workflow[1]"
  },
  {
    "objectID": "project/sets/index.html",
    "href": "project/sets/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "When visualising a small number of overlapping sets, Venn diagrams work well. But what if there are more. Here’s a tidyverse approach to the exploration of sets and their intersections.\nIn Let’s Jitter I looked at a relatively simple set of cloud-service-related sales data. G-Cloud data offers a much richer source with many thousands of services documented by several thousand suppliers and hosted across myriad web pages. These services straddle many categories. I’ll use these data to explore the sets and where they cross.\nI’m going to focus on the Cloud Hosting lot. Suppliers document the services they want to offer to Public Sector buyers. Each supplier is free to assign each of their services to one or more service categories. It would be interesting to see how these categories overlap when looking at the aggregated data.\nI’ll begin by harvesting the URL for each category’s search results. And I’ll also capture the number of search pages for each category. This will enable me to later control how R iterates through the web pages to extract the required data.\nSo now I’m all set to parallel process through the data at two levels. At category level. And within each category, I’ll iterate through the multiple pages of search results, harvesting 100 service IDs per page.\nI’ll also auto-abbreviate the category names so I’ll have the option of more concise names for less-cluttered plotting later on.\nRegex is used here to isolate pieces of text:\nNow that I have a nice tidy tibble (Müller and Wickham 2022), I can start to think about visualisations.\nI like Venn diagrams. But to create one I’ll first need to do a little prep as ggVennDiagram (Gao 2022) requires separate character vectors for each set.\nVenn diagrams work best with a small number of sets. So we’ll select four categories.\nLet’s suppose I want to find out which Service IDs lie in a particular intersection. Perhaps I want to go back to the web site with those IDs to search for, and read up on, those particular services. I could use purrr’s reduce to achieve this. For example, let’s extract the IDs at the heart of the Venn which intersect all categories.\nAnd if we wanted the IDs intersecting the “OBS” and “IND” categories?\nSometimes though we need something a little more scalable than a Venn diagram. The ggupset package provides a good solution. Before we try more than four sets though, I’ll first use the same four categories so we may compare the visualisation to the Venn.\nNow let’s take a look at the intersections across all the categories. And let’s suppose that our particular interest is all services which appear in one, and only one, category.\nSuppose we want to extract the intersection data for the top intersections across all sets. I could use functions from the tidyr package to achieve this.\nAnd I can compare this table to the equivalent ggupset (Ahlmann-Eltze 2020)visualisation.\nAnd if I want to extract all the service IDs for the top 5 intersections, I could use dplyr (Wickham et al. 2022) and tidyr (Wickham and Girlich 2022) verbs to achieve this too.\nI won’t print them all out though!"
  },
  {
    "objectID": "project/sets/index.html#r-toolbox",
    "href": "project/sets/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nabbreviate[1]; as.character[1]; c[7]; cat[6]; character[1]; function[2]; library[8]; list[5]; rep[1]\n\n\ndplyr\nfilter[7]; add_count[1]; arrange[2]; count[2]; dense_rank[1]; desc[3]; distinct[4]; group_by[3]; group_keys[1]; group_split[1]; id[7]; if_else[1]; mutate[14]; n[8]; n_distinct[2]; pull[1]; rename[1]; select[2]; slice[1]; summarise[1]; ungroup[1]\n\n\nfurrr\nfuture_map_dfr[2]; future_pmap_dfr[1]\n\n\nfuture\nmultisession[1]; plan[1]\n\n\nggVennDiagram\nggVennDiagram[2]\n\n\nggplot2\naes[6]; element_blank[3]; expansion[3]; geom_bar[3]; geom_label[3]; ggplot[3]; labs[4]; scale_colour_manual[1]; scale_fill_gradient[1]; scale_y_continuous[3]; theme[3]; theme_bw[1]; theme_set[1]\n\n\nggupset\nscale_x_upset[3]\n\n\nglue\nglue[3]\n\n\npurrr\nmap[1]; reduce[2]; set_names[1]\n\n\nreadr\nparse_number[1]\n\n\nrvest\nhtml_attr[2]; html_elements[3]; html_text[2]; read_html[3]\n\n\nstringr\nstr_c[5]; str_extract[3]; str_remove[4]; str_replace[1]; str_replace_all[4]; str_to_title[2]; str_to_upper[1]; str_trim[1]\n\n\ntibble\ntibble[2]\n\n\ntictoc\ntic[1]; toc[1]\n\n\ntidyr\npivot_wider[2]; unite[2]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/jitter/index.html",
    "href": "project/jitter/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "Welcome to the tidyverse (Wickham et al. 2019) with data ingestion, cleaning and tidying. And some visualisations of sales data with a little jittering.\nThis first little project uses the tidyverse collection of packages to import, explore and visualise some sales data. The UK Government’s Digital Marketplace provides a rich and varied source of public data under the Open Government Licence 1.\nThe marketplace was set up with an intent to break down barriers that impede Small and Medium Enterprises (SMEs) from bidding for Public Sector contracts. So, let’s see how that’s going.\nThe tidyverse framework sits at the heart of all my data science work as evidenced in my favourite things. So I’ll begin by using two of my most used tidyverse packages (readr (Wickham, Hester, and Bryan 2022) and dplyr (Wickham et al. 2022)) to import and tidy the cloud services (G-Cloud) sales data.\nWild data are often scruffy affairs. Cleaning and tidying is a necessary first step. In the case of these data, there are characters in an otherwise numeric spend column. And the date column is a mix of two formats.\nNow we can summarise and visualise how the SME share has changed over time using the ggplot2 package.\nSales grew steadily to a cumulative £2.4B by July 2017. And as the volume of sales grew, an increasingly clearer picture of sustained growth in the SME share emerged. However, in those latter few months, SMEs lost a little ground.\nDig a little deeper, and one can also see variation by sub-sector. And that’s after setting aside those buyers with cumulative G-Cloud spend below £100k, where large enterprise suppliers are less inclined to compete.\nThe box plot, overlaid with jittered points to avoid over-plotting, shows:\nSo, irrespective of whether service integration is taken in-house or handled by a service integrator, large enterprise suppliers have much to offer:\nSMEs offer flexibility, fresh thinking and broader competition, often deploying their resources and building their mission around a narrower focus. They tend to do one thing, or a few things, exceptionally well.\nThese data are explored further in Six months later and Can Ravens Forecast."
  },
  {
    "objectID": "project/jitter/index.html#r-toolbox",
    "href": "project/jitter/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.Date[1]; as.numeric[1]; c[1]; is.na[1]; library[6]; max[1]; numeric[1]; sum[4]\n\n\nclock\ndate_format[1]; date_parse[1]\n\n\ndplyr\nfilter[1]; arrange[1]; group_by[3]; if_else[4]; mutate[4]; n[1]; pull[1]; summarise[4]; ungroup[1]\n\n\nforcats\nfct_reorder[1]\n\n\nggplot2\naes[3]; geom_boxplot[1]; geom_jitter[1]; geom_label[1]; geom_point[1]; geom_smooth[1]; ggplot[2]; labs[2]; scale_x_date[1]; scale_y_continuous[2]; theme_bw[1]; theme_set[1]\n\n\nglue\nglue[4]\n\n\njanitor\nclean_names[1]\n\n\nreadr\nparse_number[1]; read_csv[1]\n\n\nscales\nlabel_percent[2]\n\n\nstats\nmedian[1]\n\n\nstringr\nstr_c[1]; str_remove_all[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/six/index.html",
    "href": "project/six/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "In September 2017 I wrote a post entitled Let’s Jitter. It looked at how the SME share of sales evolved over time. I revisited this topic here six months later, along the way adding a splash of colour and some faceted plots.\nI’m using one of the beautiful range of Wes Anderson Palettes. I often use the palettes provided by ColorBrewer too.\nIn Let’s Jitter I assumed the G-Cloud data file adopted the UK Government standard of UTF-8. I used the stringr package to fix any issues.\nThis time around, I’m importing the files for two frameworks (G-Cloud and DOS) after first checking the encoding to see if I can get a cleaner import. guess_encoding suggests these files use the ISO-8859-1 standard.\nNext I’ll set up a vector of column names to apply consistently to both files, import the data with the suggested encoding, and bind them into one tibble.\nI’d like to create some new features: Month-end dates, something to distinguish between the two frameworks (G-Cloud or DOS) and the framework version (i.e. G-Cloud 1 to 9). The spend has a messy format and needs a bit of cleaning too.\nFinding the interval between G-Cloud versions will enable me to calculate and print the average in the next paragraph using inline r code.\nEvery 7.8 months, on average, suppliers are asked to resubmit their G-Cloud offerings with their latest pricing and service descriptions. It’s a chance for new suppliers, often smaller ones, to join the existing list of suppliers and increase overall competitiveness for Cloud services.\nLet’s visualise how each of these framework versions grows, matures and fades away as the next one takes over.\nLet’s Jitter showed signs of a weakening in the SME share of G-Cloud sales by value. This plot shows this trend to have persisted, and also reflects the Digital Outcomes & Specialists (DOS) framework exhibiting a downward trend.\nOverall spending via the combined frameworks however continues to grow across all parts of Public Sector. I’ll use a small multiples visualisation technique to show this using ggplot2’s(Wickham 2016) facet_wrap.\nThe decline in the proportion of spend via SMEs is also fairly broad-based."
  },
  {
    "objectID": "project/six/index.html#r-toolbox",
    "href": "project/six/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[9]; is.na[1]; library[4]; min[1]; months[1]; sum[6]; version[2]\n\n\nclock\nadd_days[1]; add_months[1]; date_parse[1]\n\n\ndplyr\nfilter[3]; bind_rows[1]; if_else[2]; lead[1]; mutate[3]; summarise[4]\n\n\nggplot2\naes[4]; facet_wrap[2]; geom_line[4]; geom_smooth[4]; ggplot[4]; labs[8]; scale_colour_manual[4]; scale_x_date[4]; scale_y_continuous[4]; theme[2]; theme_bw[1]; theme_set[1]\n\n\npurrr\nmap[2]; set_names[1]\n\n\nreadr\nguess_encoding[1]; locale[1]; parse_number[1]; read_csv[1]\n\n\nscales\nlabel_dollar[2]; label_percent[2]\n\n\nstats\nstart[1]\n\n\nstringr\nfixed[5]; str_c[3]; str_extract[1]; str_remove[2]; str_replace[4]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/storm/index.html",
    "href": "project/storm/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "In 2020, Covid-19 began battering financial markets now further impacted by the war in Ukraine. Which sectors are faring best?\nI’ll compare each sector in the S&P 500 with the overall market. Baselining each at zero as of February 19th, we’ll see which were the first to recover lost ground.\nPerhaps not too surprising to see that Tech led the way back from Covid. But with the further impact of the situation in Ukraine, the Energy sector is now the strongest performer relative to February 2020. Comms, with all that home-working, benefited initially during the lockdown, but has faded since."
  },
  {
    "objectID": "project/storm/index.html#r-toolbox",
    "href": "project/storm/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nc[1]; library[6]; sign[1]\n\n\nclock\ndate_format[1]; date_parse[1]\n\n\ndplyr\ncase_when[1]; if_else[1]; mutate[2]\n\n\nforcats\nfct_reorder[1]\n\n\nggplot2\naes[1]; element_text[1]; facet_wrap[1]; geom_hline[1]; geom_line[1]; ggplot[1]; labs[1]; scale_colour_gradient[1]; scale_y_continuous[1]; theme[1]; theme_bw[1]; theme_set[1]\n\n\nglue\nglue[2]\n\n\nscales\nlabel_percent[1]\n\n\nstringr\nstr_wrap[1]\n\n\ntidyquant\ntq_get[1]\n\n\ntidyr\ndrop_na[1]\n\n\nwesanderson\nwes_palette[1]"
  },
  {
    "objectID": "project/footnote/index.html",
    "href": "project/footnote/index.html",
    "title": "Toolbox",
    "section": "",
    "text": "The nature of employment has seen significant shifts over time. Occupations are being consigned to ‘footnotes in history’ whilst others grow driven by trends such as concern for the environment.\nProducing a journal-quality table requires fine-grained and reproducible control over presentation. Surgical targeting of footnotes, capable of adapting to changes in the underlying data, is one example.\nThis post briefly explores the shifts in the nature of employment whilst at the same time more fully exploring the grammar of tables gt(Iannone et al. 2022): The natural companion to the grammar of graphics ggplot2(Wickham 2016).\nIn Digging Deep, the DT package is used to produce a reactable table; one with sortable and searchable columns. DT is intended as an R interface to the DataTables library, but reactivity is not yet supported in gt.\nData frames are liberally printed across all projects including, for example, a table to summarise an auto-generated overview of R packages and functions used in each project. The YAML option df-print: kable renders a nice table (with striped rows) in these cases.\nFor this project something a little more sophisticated is needed.\nAs a guiding principle, Posit packages are my first port of call. This provides a confidence in cross-package consistency, interoperability, longevity and an investment in development and support. Hence gt is the go-to package for the footnoted table further down.\nAs the intent is to present a summary in the style of the Financial Times, we’ll need a suitable custom colour palette.\nThe labour market data are sourced from the Office for National Statistics1.\nThere’s a hierarchy to the data, so I’ll extract the lowest level and then slice off the top and bottom occupations based on their percentage change over time.\nThe handling of footnotes is a particularly nice feature in gt: The package automatically assigns, and maintains the order of, the superscripted numbers (could also be symbols) to ensure they flow naturally. And targeting offers a high degree of control and reproducibility.\nFor example, two entries (highlighted light blue) in the table below use the abbreviation n.e.c.. The footnote may be targeted at rows which contain that string rather than having to manually identify the rows. And once added, any subsequent footnotes would be renumbered to maintain the flow. So, if I were to change the source datasets to different years or countries, all references to n.e.c. would be auto-magically found and appropriately footnoted.\nThe above table uses one of the in-built style theme options. It looks clean and polished. But sometimes the table to be published needs a high degree of customisation to match, for example, a specific branding. gt offers this as we’ll demonstrate by attempting to replicate the style employed by the market data in the Financial Times."
  },
  {
    "objectID": "project/footnote/index.html#r-toolbox",
    "href": "project/footnote/index.html#r-toolbox",
    "title": "Toolbox",
    "section": "R Toolbox",
    "text": "R Toolbox\nSummarising below the packages and functions used in this post enables me to separately create a toolbox visualisation summarising the usage of packages and functions across all posts.\n\n\n\n\n\n\n\n\n\nPackage\nFunction\n\n\n\nbase\nas.character[1]; as.integer[1]; c[8]; character[1]; integer[1]; is.na[1]; library[3]; list[5]\n\n\ndplyr\nfilter[1]; arrange[1]; desc[1]; if_else[1]; mutate[3]; n[2]; relocate[1]; row_number[1]; slice[1]\n\n\nforcats\nfct_inorder[1]\n\n\nggplot2\naes[2]; annotate[1]; geom_col[1]; geom_label[1]; ggplot[1]; scale_fill_manual[1]; theme[1]; theme_bw[1]; theme_set[1]; theme_void[1]\n\n\ngt\nas_raw_html[2]; cell_borders[2]; cell_fill[1]; cell_text[9]; cells_body[4]; cells_column_labels[4]; cells_column_spanners[2]; cells_footnotes[1]; cells_row_groups[4]; cells_source_notes[1]; cells_stub[3]; cells_title[1]; fmt_number[1]; fmt_percent[1]; gt[2]; html[1]; local_image[1]; opt_stylize[1]; pct[1]; px[5]; sub_missing[1]; tab_footnote[5]; tab_header[2]; tab_options[2]; tab_source_note[1]; tab_spanner[1]; tab_style[10]; tab_style_body[1]\n\n\npurrr\nlist_rbind[1]; map[1]\n\n\nreadxl\nread_xlsx[1]\n\n\nstringr\nstr_remove[1]; str_starts[1]\n\n\ntibble\ntibble[1]\n\n\ntidyr\npivot_wider[1]; separate[1]"
  }
]